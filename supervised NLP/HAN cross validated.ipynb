{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6144)])\n",
    "logical_gpus = tf.config.experimental.list_logical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "SEED = 42\n",
    "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "os.environ['HOROVOD_FUSION_THRESHOLD']='0'\n",
    "os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "\n",
    "import tfdeterminism\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import skmultilearn\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "import numpy as np\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.2.0\n",
      "Numpy version: 1.19.2\n",
      "tfdeterminism version 0.3.0\n",
      "skmultilearn version 0.2.0\n"
     ]
    }
   ],
   "source": [
    "print (\"Tensorflow version:\", tf.__version__)\n",
    "print (\"Numpy version:\", np.__version__)\n",
    "print (\"tfdeterminism version\", tfdeterminism.__version__)\n",
    "print (\"skmultilearn version\", \"0.2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone https://github.com/luinardi/hypermapper.git\n",
    "test_set = pd.read_csv('sets/Set 0.csv')\n",
    "set_1 = pd.read_csv('sets/Set 1.csv')\n",
    "set_2 = pd.read_csv('sets/Set 2.csv')\n",
    "set_3 = pd.read_csv('sets/Set 3.csv')\n",
    "set_4 = pd.read_csv('sets/Set 4.csv')\n",
    "\n",
    "\n",
    "def string_to_array_serie(serie):\n",
    "    return [[int(i) for i in t.replace(\"[\", \"\").replace(\"]\", \"\").split()] for t in serie]\n",
    "\n",
    "test_set[\"Senado\"] = string_to_array_serie(test_set[\"Senado\"])\n",
    "set_1[\"Senado\"] = string_to_array_serie(set_1[\"Senado\"])\n",
    "set_2[\"Senado\"] = string_to_array_serie(set_2[\"Senado\"])\n",
    "set_3[\"Senado\"] = string_to_array_serie(set_3[\"Senado\"])\n",
    "set_4[\"Senado\"] = string_to_array_serie(set_4[\"Senado\"])\n",
    "\n",
    "test_set[\"Referenda\"] = string_to_array_serie(test_set[\"Referenda\"])\n",
    "set_1[\"Referenda\"] = string_to_array_serie(set_1[\"Referenda\"])\n",
    "set_2[\"Referenda\"] = string_to_array_serie(set_2[\"Referenda\"])\n",
    "set_3[\"Referenda\"] = string_to_array_serie(set_3[\"Referenda\"])\n",
    "set_4[\"Referenda\"] = string_to_array_serie(set_4[\"Referenda\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Numero ato</th>\n",
       "      <th>Ementa + texto completo</th>\n",
       "      <th>Senado</th>\n",
       "      <th>Referenda</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10095</td>\n",
       "      <td>dispõe sobre o comitê consultivo de nanotecnol...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10092</td>\n",
       "      <td>promulga o protocolo de integração educativa e...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10085</td>\n",
       "      <td>dispõe sobre o programa forças no esporte segu...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10081</td>\n",
       "      <td>altera o decreto n 8713 de 15 de abril de 2016...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10083</td>\n",
       "      <td>autoriza o emprego das forças armadas na garan...</td>\n",
       "      <td>[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1323</th>\n",
       "      <td>3336</td>\n",
       "      <td>dá nova redação aos  arts  11 15 16 19 e 30 do...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1324</th>\n",
       "      <td>3330</td>\n",
       "      <td>dispõe sobre a redução do consumo de energia e...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1325</th>\n",
       "      <td>3338</td>\n",
       "      <td>aprova a estrutura regimental e o quadro demon...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>3328</td>\n",
       "      <td>altera o decreto 2889 de 21 12 1998 que dispõe...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1327</th>\n",
       "      <td>3331</td>\n",
       "      <td>revoga o  art  20 do decreto 2451 de 05 01 199...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5294 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Numero ato                            Ementa + texto completo  \\\n",
       "0          10095  dispõe sobre o comitê consultivo de nanotecnol...   \n",
       "1          10092  promulga o protocolo de integração educativa e...   \n",
       "2          10085  dispõe sobre o programa forças no esporte segu...   \n",
       "3          10081  altera o decreto n 8713 de 15 de abril de 2016...   \n",
       "4          10083  autoriza o emprego das forças armadas na garan...   \n",
       "...          ...                                                ...   \n",
       "1323        3336  dá nova redação aos  arts  11 15 16 19 e 30 do...   \n",
       "1324        3330  dispõe sobre a redução do consumo de energia e...   \n",
       "1325        3338  aprova a estrutura regimental e o quadro demon...   \n",
       "1326        3328  altera o decreto 2889 de 21 12 1998 que dispõe...   \n",
       "1327        3331  revoga o  art  20 do decreto 2451 de 05 01 199...   \n",
       "\n",
       "                                             Senado  \\\n",
       "0     [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0]   \n",
       "1     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0]   \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0]   \n",
       "4     [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "...                                             ...   \n",
       "1323  [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0]   \n",
       "1324  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]   \n",
       "1325  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0]   \n",
       "1326  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]   \n",
       "1327  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0]   \n",
       "\n",
       "                                    Referenda  \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0]  \n",
       "1     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "2     [0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0]  \n",
       "3     [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0]  \n",
       "4     [0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "...                                       ...  \n",
       "1323  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]  \n",
       "1324  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]  \n",
       "1325  [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0]  \n",
       "1326  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  \n",
       "1327  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]  \n",
       "\n",
       "[5294 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sets = [set_1, set_2, set_3, set_4]\n",
    "\n",
    "full_train = pd.concat(train_sets)\n",
    "full_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/caiocampos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import tokenize\n",
    "\n",
    "# max_senten_len = 40\n",
    "# max_senten_num = 48\n",
    "max_senten_len = 49\n",
    "max_senten_num = 68\n",
    "\n",
    "def preprocess_HANS(dataset, max_sentence_num, max_sentence_len, tokenizer):\n",
    "    data = np.zeros((len(dataset), max_sentence_num, max_sentence_len), dtype='int32')\n",
    "    for i, document in enumerate(dataset):\n",
    "        tokenized_document=tokenize.sent_tokenize(document)\n",
    "        for j, sent in enumerate(tokenized_document):\n",
    "            if j< max_sentence_num:\n",
    "                wordTokens = text_to_word_sequence(sent)\n",
    "                k=0\n",
    "                for _, word in enumerate(wordTokens):\n",
    "                    try:\n",
    "                        if k<max_sentence_len: #and tokenizer.word_index[word]<26486:\n",
    "                            data[i,j,k] = tokenizer.word_index[word]\n",
    "                            k=k+1\n",
    "                    except:\n",
    "                        pass\n",
    "    return data\n",
    "\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True, oov_token=\"OOV\")\n",
    "tokenizer.fit_on_texts(full_train[\"Ementa + texto completo\"])\n",
    "\n",
    "Train_X = preprocess_HANS(full_train[\"Ementa + texto completo\"], max_senten_num, max_senten_len, tokenizer)\n",
    "Test_X = preprocess_HANS(test_set[\"Ementa + texto completo\"], max_senten_num, max_senten_len, tokenizer)\n",
    "\n",
    "# The following sets will be used for validation.\n",
    "set_1_X = preprocess_HANS(set_1[\"Ementa + texto completo\"], max_senten_num, max_senten_len, tokenizer)\n",
    "set_2_X = preprocess_HANS(set_2[\"Ementa + texto completo\"], max_senten_num, max_senten_len, tokenizer)\n",
    "set_3_X = preprocess_HANS(set_3[\"Ementa + texto completo\"], max_senten_num, max_senten_len, tokenizer)\n",
    "set_4_X = preprocess_HANS(set_4[\"Ementa + texto completo\"], max_senten_num, max_senten_len, tokenizer)\n",
    "\n",
    "sets_X = [set_1_X, set_2_X, set_3_X, set_4_X]\n",
    "\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Y_sen = full_train[\"Senado\"].values\n",
    "Test_Y_sen = test_set[\"Senado\"].values\n",
    "Train_Y_minist = full_train[\"Referenda\"].values\n",
    "Test_Y_minist = test_set[\"Referenda\"].values\n",
    "\n",
    "def get_class_weights(n_classes, labels):\n",
    "    counters = np.zeros(n_classes)\n",
    "    for label in labels:\n",
    "        for i in range(len(label)):\n",
    "            if(label[i] == 1):\n",
    "                counters[i]+=1\n",
    "    greater_class = counters[np.argmax(counters)]\n",
    "    weights = []\n",
    "    for i in range(n_classes):\n",
    "        weights.append(greater_class/counters[i])\n",
    "    return weights\n",
    "\n",
    "classes_w_sen = get_class_weights(len(Train_Y_sen[0]), Train_Y_sen)\n",
    "classes_w_sen = {v: k for v, k in enumerate(classes_w_sen)}\n",
    "classes_w_minist = get_class_weights(len(Train_Y_minist[0]), Train_Y_minist)\n",
    "classes_w_minist = {v: k for v, k in enumerate(classes_w_minist)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_1_Y_sen = set_1[\"Senado\"].values\n",
    "set_2_Y_sen = set_2[\"Senado\"].values\n",
    "set_3_Y_sen = set_3[\"Senado\"].values\n",
    "set_4_Y_sen = set_4[\"Senado\"].values\n",
    "sets_Y_sen = [set_1_Y_sen, set_2_Y_sen, set_3_Y_sen, set_4_Y_sen]\n",
    "\n",
    "set_1_Y_minist = set_1[\"Referenda\"].values\n",
    "set_2_Y_minist = set_2[\"Referenda\"].values\n",
    "set_3_Y_minist = set_3[\"Referenda\"].values\n",
    "set_4_Y_minist = set_4[\"Referenda\"].values\n",
    "sets_Y_minist = [set_1_Y_minist, set_2_Y_minist, set_3_Y_minist, set_4_Y_minist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding, Input, Dense, GRU, Bidirectional, TimeDistributed\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer,  text_to_word_sequence\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializers as initializers, regularizers, constraints\n",
    "from keras.callbacks import Callback, ModelCheckpoint\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r$ could not be embedded.\n",
      "00 could not be embedded.\n",
      "三藏法師玄奘奉 could not be embedded.\n",
      "r$ could not be embedded.\n",
      "\n",
      "Found 929594 word vectors.\n",
      "\n",
      "0 could not be indexed.\n",
      "00 could not be indexed.\n"
     ]
    }
   ],
   "source": [
    "#pre-trained Glove embedding\n",
    "embeddings_index = {}\n",
    "f = open('glove_s100.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    except:\n",
    "        print(word, \"could not be embedded.\")\n",
    "f.close()\n",
    "\n",
    "print('\\nFound %s word vectors.\\n' % len(embeddings_index))\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index.items())+1, 100))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        except:\n",
    "            print(word, \"could not be indexed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See https://www.kaggle.com/sermakarevich/hierarchical-attention-network\n",
    "# See https://gist.github.com/cbaziotis/7ef97ccf71cbc14366835198c09809d2\n",
    "\n",
    "\n",
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatibl|e with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "#         self.init = initializers.get('glorot_uniform')\n",
    "        self.init = initializers.glorot_uniform(seed=SEED)\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight(shape=(input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(shape=(input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight(shape=(input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Taken from old keras code. The following is used only during training for visualization.\n",
    "def get_f1(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def predict_classes(probs, tresh=0.5, consider_first=False):\n",
    "    predicted_classes = []\n",
    "    for prob in probs:\n",
    "        predicted_class = np.zeros(len(probs[0])).astype(int)\n",
    "        for i in range(len(prob)):\n",
    "            if(prob[i]>=tresh):\n",
    "                predicted_class[i] = 1\n",
    "        if np.sum(predicted_class) == 0 and consider_first:\n",
    "            position_1 = (np.argsort(prob)[::-1])[0]\n",
    "            predicted_class[position_1] = 1\n",
    "        predicted_classes.append(predicted_class.tolist())\n",
    "    \n",
    "    return predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acerto_absoluto(test_y_sen, test_pred, number_labels):\n",
    "    acerto_100 = 0\n",
    "    test_prediction_binary = predict_classes(test_pred)\n",
    "    for i in range(len(test_prediction_binary)):\n",
    "        acertos_internos = 0\n",
    "        for j in range(len(test_prediction_binary[i])):\n",
    "            if test_y_sen[i][j] == test_prediction_binary[i][j]:\n",
    "                acertos_internos+=1\n",
    "        if acertos_internos == number_labels:\n",
    "            acerto_100+=1\n",
    "    return acerto_100/len(test_prediction_binary)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create Keras functional model for hierarchical attention network\n",
    "\"\"\"\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from keras.optimizers import Adam\n",
    "import imp\n",
    "\n",
    "def HAN_model(number_of_classes):\n",
    "    K.clear_session()\n",
    "\n",
    "    os.environ['PYTHONHASHSEED']=str(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    random.seed(SEED)\n",
    "    \n",
    "    tf.random.set_seed(SEED)\n",
    "    os.environ['TF_CUDNN_DETERMINISM']='1'\n",
    "    os.environ['TF_DETERMINISTIC_OPS']='1'\n",
    "    os.environ['HOROVOD_FUSION_THRESHOLD']='0'\n",
    "    session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "    \n",
    "    \n",
    "    embedding_layer = Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_shape=(Train_X.shape[1], ) )\n",
    "\n",
    "    # Words level attention model\n",
    "    word_input = Input(shape=(max_senten_len,), dtype='int32',name='word_input')\n",
    "    word_sequences = embedding_layer(word_input)\n",
    "    word_gru = Bidirectional(GRU(50, return_sequences=True, kernel_initializer=initializers.glorot_uniform(seed=SEED)),name='word_gru')(word_sequences)\n",
    "    word_att = AttentionWithContext()(word_gru)#(word_dense)\n",
    "    wordEncoder = Model(inputs = word_input,outputs = word_att)\n",
    "\n",
    "    # Sentence level attention model\n",
    "    sent_input = Input(shape=(max_senten_num, max_senten_len), dtype='int32',name='sent_input')\n",
    "    sent_encoder = TimeDistributed(wordEncoder,name='sent_linking')(sent_input)\n",
    "    sent_gru = Bidirectional(GRU(50, return_sequences=True, kernel_initializer=initializers.glorot_uniform(seed=SEED)),name='sent_gru')(sent_encoder)\n",
    "    sent_att = AttentionWithContext()(sent_gru)#(sent_dense)\n",
    "    preds = Dense(number_of_classes, activation='sigmoid', input_shape=(Train_X.shape[1],), kernel_initializer=initializers.glorot_uniform(seed=SEED))(sent_att)\n",
    "\n",
    "    model = Model(sent_input, preds)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization with bayesian optimization and Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_classes_names = [\"Saúde\", \"Relações Exteriores\", \"Meio ambiente\", \"Educação, cultura e esporte\", \"Segurança Pública e Defesa\", \"Trabalho e Previdência\", \"Agricultura, pecuária e pesca\", \"Ciência, tecnologia e comunicações\", \"Social\", \"Indústria, comércio, turismo, transporte/transporte de mercadorias\", \"Economia, planejamento e sistema financeiro\", \"Assuntos internos,  cargos e comissões, Estado\", \"Tributos\", \"Minas e Energia\", \"Justiça e direitos\"]\n",
    "minist_classes_names = [\"Saúde\", \"Relações Exteriores\", \"Meio ambiente\", \"Educação, cultura e esporte\", \"Justiça e Segurança\", \"Trabalho e Previdência\", \"Transporte/transporte de mercadorias\", \"Agricultura, pecuária e pesca\", \"Ciência e tecnologia\", \"Social\", \"Presidência\", \"Economia e planejamento\", \"Indústria, comércio, obras públicas, turismo\"]\n",
    "stdout = sys.stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_validation_csv(f1s_per_class, precision_per_class, recall_per_class, prefix, names):\n",
    "    final_metrics = (f1s_per_class, np.array([np.mean(f1s_per_class, axis=0)]), np.array([np.std(f1s_per_class, axis=0)]),\n",
    "                     precision_per_class, np.array([np.mean(precision_per_class, axis=0)]), np.array([np.std(precision_per_class, axis=0)]),\n",
    "                     recall_per_class, np.array([np.mean(recall_per_class, axis=0)]), np.array([np.std(recall_per_class, axis=0)]) )\n",
    "    f1s_per_class_pd = pd.DataFrame(np.concatenate(final_metrics, axis=0))\n",
    "    f1s_per_class_pd.columns = names\n",
    "    f1s_per_class_pd.index=[\"validation set 1 f1\", \"validation set 2 f1\", \"validation set 3 f1\", \"validation set 4 f1\", \"validation sets mean f1\", \"validation std f1\",\n",
    "                            \"validation set 1 precision\", \"validation set 2 precision\", \"validation set 3 precision\", \"validation set 4 precision\", \"validation sets mean precision\", \"validation std precision\",\n",
    "                            \"validation set 1 recall\", \"validation set 2 recall\", \"validation set 3 recall\", \"validation set 4 recall\", \"validation sets mean recall\", \"validation std recall\"]\n",
    "    f1s_per_class_pd.to_csv(\"../optimization/validation_results/\"+prefix+\"iteration_\"+str(iterations)+\".csv\", index=True, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Senado - otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "os.chdir(\"/home/caiocampos/andre/radar_wisemap\")\n",
    "sys.path.append('hypermapper/scripts/')\n",
    "import hypermapper\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "def optimize_HAN(parameters):\n",
    "    global iterations\n",
    "    f1s = 0\n",
    "    f1s_per_class = []\n",
    "    precision_per_class = []\n",
    "    recall_per_class = []\n",
    "    beta_2_real_p = [0.99,0.999,0.9999]  # O parâmetro beta2 é traduzido de categórico para um dos três reais descritos no paper do Adam.\n",
    "    \n",
    "    for i in range(len(train_sets)):        \n",
    "        sets_index = []\n",
    "        for t in range(len(train_sets)):\n",
    "            if t!=i:\n",
    "                sets_index.append(t)\n",
    "\n",
    "        train_X = np.concatenate(np.array(sets_X)[sets_index])\n",
    "        validation_X = sets_X[i]\n",
    "\n",
    "        train_Y_sen = np.concatenate(np.array(sets_Y_sen)[sets_index])\n",
    "        train_Y_sen = np.array([np.array(t) for t in train_Y_sen])\n",
    "        validation_Y_sen = sets_Y_sen[i]\n",
    "        validation_Y_sen = np.array([np.array(v) for v in validation_Y_sen])\n",
    "\n",
    "        model = HAN_model(len(Train_Y_sen[0]))\n",
    "        opt = Adam(lr=parameters['learning_rate'], beta_1=parameters['beta1'], beta_2=beta_2_real_p[parameters['beta2']])\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1])\n",
    "        history = model.fit(x=np.array(train_X), y=np.array(train_Y_sen), epochs=parameters['epochs'], validation_split=0.0, class_weight=classes_w_sen, batch_size=parameters['batch_size'], shuffle=False, verbose=0)\n",
    "        validation_prediction = model.predict(np.array(validation_X))\n",
    "\n",
    "        f1s+=f1_score(y_true=validation_Y_sen, y_pred=predict_classes(validation_prediction), average='macro')\n",
    "        f1s_per_class.append(f1_score(y_true=validation_Y_sen, y_pred=predict_classes(validation_prediction), average=None))\n",
    "        precision_per_class.append(precision_score(y_true=validation_Y_sen, y_pred=predict_classes(validation_prediction), average=None))\n",
    "        recall_per_class.append(recall_score(y_true=validation_Y_sen, y_pred=predict_classes(validation_prediction), average=None))\n",
    "    generate_validation_csv(f1s_per_class, precision_per_class, recall_per_class, \"HAN_sen_\", sen_classes_names)\n",
    "    f1_loss=1-f1s/4\n",
    "    print(\"Iteration \"+str(iterations)+\":\\nlearning_rate: \", parameters['learning_rate'], \" || beta1: \", parameters['beta1'], \" || beta2: \", beta_2_real_p[parameters['beta2']], \" || epochs: \", parameters['epochs'], \" || batch_size: \", parameters['batch_size'], \" || (1 - macro_F1): \", f1_loss)\n",
    "    iterations+=1\n",
    "    return f1_loss\n",
    "\n",
    "os.chdir(\"hypermapper/\")\n",
    "stdout = sys.stdout\n",
    "print(os.getcwd())\n",
    "hypermapper.optimize(\"../optimization/HAN_scenario.json\", optimize_HAN)\n",
    "os.chdir(\"/home/caiocampos/andre/radar_wisemap/\")\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>beta1</th>\n",
       "      <th>beta2</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>1 - F1</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>0.007347</td>\n",
       "      <td>0.802171</td>\n",
       "      <td>1.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.199191</td>\n",
       "      <td>37732822.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate     beta1  beta2  epochs  batch_size    1 - F1   Timestamp\n",
       "90       0.007347  0.802171    1.0    21.0        45.0  0.199191  37732822.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimization_results = pd.read_csv(\"optimization/sen_han_validation_hypermapper_output.csv\")\n",
    "best_parameters_idx = optimization_results[\"1 - F1\"].idxmin()\n",
    "parameters = pd.DataFrame(optimization_results.iloc[best_parameters_idx]).transpose()\n",
    "parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Saúde</th>\n",
       "      <th>Relações Exteriores</th>\n",
       "      <th>Meio ambiente</th>\n",
       "      <th>Educação, cultura e esporte</th>\n",
       "      <th>Segurança Pública e Defesa</th>\n",
       "      <th>Trabalho e Previdência</th>\n",
       "      <th>Agricultura, pecuária e pesca</th>\n",
       "      <th>Ciência, tecnologia e comunicações</th>\n",
       "      <th>Social</th>\n",
       "      <th>Indústria, comércio, turismo, transporte/transporte de mercadorias</th>\n",
       "      <th>Economia, planejamento e sistema financeiro</th>\n",
       "      <th>Assuntos internos,  cargos e comissões, Estado</th>\n",
       "      <th>Tributos</th>\n",
       "      <th>Minas e Energia</th>\n",
       "      <th>Justiça e direitos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>validation set 1 f1</th>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.965636</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.753623</td>\n",
       "      <td>0.770000</td>\n",
       "      <td>0.637931</td>\n",
       "      <td>0.805755</td>\n",
       "      <td>0.864017</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.613333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 2 f1</th>\n",
       "      <td>0.615385</td>\n",
       "      <td>0.956672</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.822430</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.701031</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.731518</td>\n",
       "      <td>0.862106</td>\n",
       "      <td>0.887500</td>\n",
       "      <td>0.873563</td>\n",
       "      <td>0.651163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 3 f1</th>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.958621</td>\n",
       "      <td>0.820513</td>\n",
       "      <td>0.783505</td>\n",
       "      <td>0.858586</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.863158</td>\n",
       "      <td>0.840580</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.715447</td>\n",
       "      <td>0.737643</td>\n",
       "      <td>0.864461</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.607595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 4 f1</th>\n",
       "      <td>0.738462</td>\n",
       "      <td>0.960413</td>\n",
       "      <td>0.865672</td>\n",
       "      <td>0.862745</td>\n",
       "      <td>0.847291</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.722892</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.805861</td>\n",
       "      <td>0.869927</td>\n",
       "      <td>0.890173</td>\n",
       "      <td>0.891566</td>\n",
       "      <td>0.682927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation sets mean f1</th>\n",
       "      <td>0.694736</td>\n",
       "      <td>0.960335</td>\n",
       "      <td>0.806271</td>\n",
       "      <td>0.807641</td>\n",
       "      <td>0.846362</td>\n",
       "      <td>0.845157</td>\n",
       "      <td>0.793321</td>\n",
       "      <td>0.810612</td>\n",
       "      <td>0.738339</td>\n",
       "      <td>0.688345</td>\n",
       "      <td>0.770194</td>\n",
       "      <td>0.865128</td>\n",
       "      <td>0.885391</td>\n",
       "      <td>0.861542</td>\n",
       "      <td>0.638754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation std f1</th>\n",
       "      <td>0.052350</td>\n",
       "      <td>0.003334</td>\n",
       "      <td>0.040274</td>\n",
       "      <td>0.032487</td>\n",
       "      <td>0.014485</td>\n",
       "      <td>0.017775</td>\n",
       "      <td>0.049797</td>\n",
       "      <td>0.033728</td>\n",
       "      <td>0.041156</td>\n",
       "      <td>0.037980</td>\n",
       "      <td>0.035680</td>\n",
       "      <td>0.002909</td>\n",
       "      <td>0.006073</td>\n",
       "      <td>0.042253</td>\n",
       "      <td>0.030505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 1 precision</th>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.972318</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.837838</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.787879</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.660714</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.878723</td>\n",
       "      <td>0.958904</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.821429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 2 precision</th>\n",
       "      <td>0.551724</td>\n",
       "      <td>0.975265</td>\n",
       "      <td>0.827586</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.807339</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.925926</td>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.672131</td>\n",
       "      <td>0.789916</td>\n",
       "      <td>0.918981</td>\n",
       "      <td>0.959459</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.736842</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 3 precision</th>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.972028</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.934066</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.707547</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.769841</td>\n",
       "      <td>0.898004</td>\n",
       "      <td>0.894118</td>\n",
       "      <td>0.840909</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 4 precision</th>\n",
       "      <td>0.648649</td>\n",
       "      <td>0.968750</td>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.897959</td>\n",
       "      <td>0.895833</td>\n",
       "      <td>0.758621</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.804124</td>\n",
       "      <td>0.771930</td>\n",
       "      <td>0.802920</td>\n",
       "      <td>0.906725</td>\n",
       "      <td>0.927711</td>\n",
       "      <td>0.860465</td>\n",
       "      <td>0.823529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation sets mean precision</th>\n",
       "      <td>0.674876</td>\n",
       "      <td>0.972090</td>\n",
       "      <td>0.830429</td>\n",
       "      <td>0.822696</td>\n",
       "      <td>0.868769</td>\n",
       "      <td>0.823178</td>\n",
       "      <td>0.827772</td>\n",
       "      <td>0.848829</td>\n",
       "      <td>0.763235</td>\n",
       "      <td>0.715849</td>\n",
       "      <td>0.790669</td>\n",
       "      <td>0.900608</td>\n",
       "      <td>0.935048</td>\n",
       "      <td>0.837546</td>\n",
       "      <td>0.782950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation std precision</th>\n",
       "      <td>0.082506</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.046080</td>\n",
       "      <td>0.044312</td>\n",
       "      <td>0.049311</td>\n",
       "      <td>0.069981</td>\n",
       "      <td>0.030629</td>\n",
       "      <td>0.050215</td>\n",
       "      <td>0.036528</td>\n",
       "      <td>0.049814</td>\n",
       "      <td>0.012957</td>\n",
       "      <td>0.014669</td>\n",
       "      <td>0.026899</td>\n",
       "      <td>0.027502</td>\n",
       "      <td>0.039809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 1 recall</th>\n",
       "      <td>0.730769</td>\n",
       "      <td>0.959044</td>\n",
       "      <td>0.742857</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.877358</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.787234</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.616667</td>\n",
       "      <td>0.811594</td>\n",
       "      <td>0.849794</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.731707</td>\n",
       "      <td>0.489362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 2 recall</th>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.938776</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.784314</td>\n",
       "      <td>0.838095</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.735294</td>\n",
       "      <td>0.653846</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.681159</td>\n",
       "      <td>0.811861</td>\n",
       "      <td>0.825581</td>\n",
       "      <td>0.974359</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 3 recall</th>\n",
       "      <td>0.629630</td>\n",
       "      <td>0.945578</td>\n",
       "      <td>0.842105</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.794393</td>\n",
       "      <td>0.960000</td>\n",
       "      <td>0.854167</td>\n",
       "      <td>0.852941</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.676923</td>\n",
       "      <td>0.708029</td>\n",
       "      <td>0.833333</td>\n",
       "      <td>0.883721</td>\n",
       "      <td>0.948718</td>\n",
       "      <td>0.510638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 4 recall</th>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.952218</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.830189</td>\n",
       "      <td>0.803738</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.638298</td>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.772277</td>\n",
       "      <td>0.698413</td>\n",
       "      <td>0.808824</td>\n",
       "      <td>0.836000</td>\n",
       "      <td>0.855556</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation sets mean recall</th>\n",
       "      <td>0.728298</td>\n",
       "      <td>0.948904</td>\n",
       "      <td>0.785202</td>\n",
       "      <td>0.793626</td>\n",
       "      <td>0.828396</td>\n",
       "      <td>0.880224</td>\n",
       "      <td>0.765577</td>\n",
       "      <td>0.779003</td>\n",
       "      <td>0.715711</td>\n",
       "      <td>0.663323</td>\n",
       "      <td>0.752402</td>\n",
       "      <td>0.832747</td>\n",
       "      <td>0.842364</td>\n",
       "      <td>0.894946</td>\n",
       "      <td>0.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation std recall</th>\n",
       "      <td>0.082777</td>\n",
       "      <td>0.007541</td>\n",
       "      <td>0.050665</td>\n",
       "      <td>0.025470</td>\n",
       "      <td>0.032618</td>\n",
       "      <td>0.070780</td>\n",
       "      <td>0.078752</td>\n",
       "      <td>0.053166</td>\n",
       "      <td>0.049274</td>\n",
       "      <td>0.029988</td>\n",
       "      <td>0.058591</td>\n",
       "      <td>0.013581</td>\n",
       "      <td>0.029968</td>\n",
       "      <td>0.095849</td>\n",
       "      <td>0.042340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Saúde  Relações Exteriores  Meio ambiente  \\\n",
       "validation set 1 f1             0.745098             0.965636       0.764706   \n",
       "validation set 2 f1             0.615385             0.956672       0.774194   \n",
       "validation set 3 f1             0.680000             0.958621       0.820513   \n",
       "validation set 4 f1             0.738462             0.960413       0.865672   \n",
       "validation sets mean f1         0.694736             0.960335       0.806271   \n",
       "validation std f1               0.052350             0.003334       0.040274   \n",
       "validation set 1 precision      0.760000             0.972318       0.787879   \n",
       "validation set 2 precision      0.551724             0.975265       0.827586   \n",
       "validation set 3 precision      0.739130             0.972028       0.800000   \n",
       "validation set 4 precision      0.648649             0.968750       0.906250   \n",
       "validation sets mean precision  0.674876             0.972090       0.830429   \n",
       "validation std precision        0.082506             0.002307       0.046080   \n",
       "validation set 1 recall         0.730769             0.959044       0.742857   \n",
       "validation set 2 recall         0.695652             0.938776       0.727273   \n",
       "validation set 3 recall         0.629630             0.945578       0.842105   \n",
       "validation set 4 recall         0.857143             0.952218       0.828571   \n",
       "validation sets mean recall     0.728298             0.948904       0.785202   \n",
       "validation std recall           0.082777             0.007541       0.050665   \n",
       "\n",
       "                                Educação, cultura e esporte  \\\n",
       "validation set 1 f1                                0.800000   \n",
       "validation set 2 f1                                0.784314   \n",
       "validation set 3 f1                                0.783505   \n",
       "validation set 4 f1                                0.862745   \n",
       "validation sets mean f1                            0.807641   \n",
       "validation std f1                                  0.032487   \n",
       "validation set 1 precision                         0.800000   \n",
       "validation set 2 precision                         0.784314   \n",
       "validation set 3 precision                         0.808511   \n",
       "validation set 4 precision                         0.897959   \n",
       "validation sets mean precision                     0.822696   \n",
       "validation std precision                           0.044312   \n",
       "validation set 1 recall                            0.800000   \n",
       "validation set 2 recall                            0.784314   \n",
       "validation set 3 recall                            0.760000   \n",
       "validation set 4 recall                            0.830189   \n",
       "validation sets mean recall                        0.793626   \n",
       "validation std recall                              0.025470   \n",
       "\n",
       "                                Segurança Pública e Defesa  \\\n",
       "validation set 1 f1                               0.857143   \n",
       "validation set 2 f1                               0.822430   \n",
       "validation set 3 f1                               0.858586   \n",
       "validation set 4 f1                               0.847291   \n",
       "validation sets mean f1                           0.846362   \n",
       "validation std f1                                 0.014485   \n",
       "validation set 1 precision                        0.837838   \n",
       "validation set 2 precision                        0.807339   \n",
       "validation set 3 precision                        0.934066   \n",
       "validation set 4 precision                        0.895833   \n",
       "validation sets mean precision                    0.868769   \n",
       "validation std precision                          0.049311   \n",
       "validation set 1 recall                           0.877358   \n",
       "validation set 2 recall                           0.838095   \n",
       "validation set 3 recall                           0.794393   \n",
       "validation set 4 recall                           0.803738   \n",
       "validation sets mean recall                       0.828396   \n",
       "validation std recall                             0.032618   \n",
       "\n",
       "                                Trabalho e Previdência  \\\n",
       "validation set 1 f1                           0.875000   \n",
       "validation set 2 f1                           0.833333   \n",
       "validation set 3 f1                           0.842105   \n",
       "validation set 4 f1                           0.830189   \n",
       "validation sets mean f1                       0.845157   \n",
       "validation std f1                             0.017775   \n",
       "validation set 1 precision                    0.875000   \n",
       "validation set 2 precision                    0.909091   \n",
       "validation set 3 precision                    0.750000   \n",
       "validation set 4 precision                    0.758621   \n",
       "validation sets mean precision                0.823178   \n",
       "validation std precision                      0.069981   \n",
       "validation set 1 recall                       0.875000   \n",
       "validation set 2 recall                       0.769231   \n",
       "validation set 3 recall                       0.960000   \n",
       "validation set 4 recall                       0.916667   \n",
       "validation sets mean recall                   0.880224   \n",
       "validation std recall                         0.070780   \n",
       "\n",
       "                                Agricultura, pecuária e pesca  \\\n",
       "validation set 1 f1                                  0.787234   \n",
       "validation set 2 f1                                  0.800000   \n",
       "validation set 3 f1                                  0.863158   \n",
       "validation set 4 f1                                  0.722892   \n",
       "validation sets mean f1                              0.793321   \n",
       "validation std f1                                    0.049797   \n",
       "validation set 1 precision                           0.787234   \n",
       "validation set 2 precision                           0.818182   \n",
       "validation set 3 precision                           0.872340   \n",
       "validation set 4 precision                           0.833333   \n",
       "validation sets mean precision                       0.827772   \n",
       "validation std precision                             0.030629   \n",
       "validation set 1 recall                              0.787234   \n",
       "validation set 2 recall                              0.782609   \n",
       "validation set 3 recall                              0.854167   \n",
       "validation set 4 recall                              0.638298   \n",
       "validation sets mean recall                          0.765577   \n",
       "validation std recall                                0.078752   \n",
       "\n",
       "                                Ciência, tecnologia e comunicações    Social  \\\n",
       "validation set 1 f1                                       0.753623  0.770000   \n",
       "validation set 2 f1                                       0.819672  0.701031   \n",
       "validation set 3 f1                                       0.840580  0.694444   \n",
       "validation set 4 f1                                       0.828571  0.787879   \n",
       "validation sets mean f1                                   0.810612  0.738339   \n",
       "validation std f1                                         0.033728  0.041156   \n",
       "validation set 1 precision                                0.787879  0.785714   \n",
       "validation set 2 precision                                0.925926  0.755556   \n",
       "validation set 3 precision                                0.828571  0.707547   \n",
       "validation set 4 precision                                0.852941  0.804124   \n",
       "validation sets mean precision                            0.848829  0.763235   \n",
       "validation std precision                                  0.050215  0.036528   \n",
       "validation set 1 recall                                   0.722222  0.754902   \n",
       "validation set 2 recall                                   0.735294  0.653846   \n",
       "validation set 3 recall                                   0.852941  0.681818   \n",
       "validation set 4 recall                                   0.805556  0.772277   \n",
       "validation sets mean recall                               0.779003  0.715711   \n",
       "validation std recall                                     0.053166  0.049274   \n",
       "\n",
       "                                Indústria, comércio, turismo, transporte/transporte de mercadorias  \\\n",
       "validation set 1 f1                                                      0.637931                    \n",
       "validation set 2 f1                                                      0.666667                    \n",
       "validation set 3 f1                                                      0.715447                    \n",
       "validation set 4 f1                                                      0.733333                    \n",
       "validation sets mean f1                                                  0.688345                    \n",
       "validation std f1                                                        0.037980                    \n",
       "validation set 1 precision                                               0.660714                    \n",
       "validation set 2 precision                                               0.672131                    \n",
       "validation set 3 precision                                               0.758621                    \n",
       "validation set 4 precision                                               0.771930                    \n",
       "validation sets mean precision                                           0.715849                    \n",
       "validation std precision                                                 0.049814                    \n",
       "validation set 1 recall                                                  0.616667                    \n",
       "validation set 2 recall                                                  0.661290                    \n",
       "validation set 3 recall                                                  0.676923                    \n",
       "validation set 4 recall                                                  0.698413                    \n",
       "validation sets mean recall                                              0.663323                    \n",
       "validation std recall                                                    0.029988                    \n",
       "\n",
       "                                Economia, planejamento e sistema financeiro  \\\n",
       "validation set 1 f1                                                0.805755   \n",
       "validation set 2 f1                                                0.731518   \n",
       "validation set 3 f1                                                0.737643   \n",
       "validation set 4 f1                                                0.805861   \n",
       "validation sets mean f1                                            0.770194   \n",
       "validation std f1                                                  0.035680   \n",
       "validation set 1 precision                                         0.800000   \n",
       "validation set 2 precision                                         0.789916   \n",
       "validation set 3 precision                                         0.769841   \n",
       "validation set 4 precision                                         0.802920   \n",
       "validation sets mean precision                                     0.790669   \n",
       "validation std precision                                           0.012957   \n",
       "validation set 1 recall                                            0.811594   \n",
       "validation set 2 recall                                            0.681159   \n",
       "validation set 3 recall                                            0.708029   \n",
       "validation set 4 recall                                            0.808824   \n",
       "validation sets mean recall                                        0.752402   \n",
       "validation std recall                                              0.058591   \n",
       "\n",
       "                                Assuntos internos,  cargos e comissões, Estado  \\\n",
       "validation set 1 f1                                                   0.864017   \n",
       "validation set 2 f1                                                   0.862106   \n",
       "validation set 3 f1                                                   0.864461   \n",
       "validation set 4 f1                                                   0.869927   \n",
       "validation sets mean f1                                               0.865128   \n",
       "validation std f1                                                     0.002909   \n",
       "validation set 1 precision                                            0.878723   \n",
       "validation set 2 precision                                            0.918981   \n",
       "validation set 3 precision                                            0.898004   \n",
       "validation set 4 precision                                            0.906725   \n",
       "validation sets mean precision                                        0.900608   \n",
       "validation std precision                                              0.014669   \n",
       "validation set 1 recall                                               0.849794   \n",
       "validation set 2 recall                                               0.811861   \n",
       "validation set 3 recall                                               0.833333   \n",
       "validation set 4 recall                                               0.836000   \n",
       "validation sets mean recall                                           0.832747   \n",
       "validation std recall                                                 0.013581   \n",
       "\n",
       "                                Tributos  Minas e Energia  Justiça e direitos  \n",
       "validation set 1 f1             0.875000         0.789474            0.613333  \n",
       "validation set 2 f1             0.887500         0.873563            0.651163  \n",
       "validation set 3 f1             0.888889         0.891566            0.607595  \n",
       "validation set 4 f1             0.890173         0.891566            0.682927  \n",
       "validation sets mean f1         0.885391         0.861542            0.638754  \n",
       "validation std f1               0.006073         0.042253            0.030505  \n",
       "validation set 1 precision      0.958904         0.857143            0.821429  \n",
       "validation set 2 precision      0.959459         0.791667            0.736842  \n",
       "validation set 3 precision      0.894118         0.840909            0.750000  \n",
       "validation set 4 precision      0.927711         0.860465            0.823529  \n",
       "validation sets mean precision  0.935048         0.837546            0.782950  \n",
       "validation std precision        0.026899         0.027502            0.039809  \n",
       "validation set 1 recall         0.804598         0.731707            0.489362  \n",
       "validation set 2 recall         0.825581         0.974359            0.583333  \n",
       "validation set 3 recall         0.883721         0.948718            0.510638  \n",
       "validation set 4 recall         0.855556         0.925000            0.583333  \n",
       "validation sets mean recall     0.842364         0.894946            0.541667  \n",
       "validation std recall           0.029968         0.095849            0.042340  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_results = pd.read_csv(\"optimization/validation_results/HAN_sen_iteration_\"+str(best_parameters_idx)+\".csv\", index_col=0)\n",
    "validation_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final senate evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "118/118 [==============================] - 9s 76ms/step - loss: 1.4186 - get_f1: 0.0466\n",
      "Epoch 2/21\n",
      "118/118 [==============================] - 9s 74ms/step - loss: 0.6491 - get_f1: 0.7113\n",
      "Epoch 3/21\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 0.2796 - get_f1: 0.8716\n",
      "Epoch 4/21\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 0.1375 - get_f1: 0.9327\n",
      "Epoch 5/21\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 0.0765 - get_f1: 0.9606\n",
      "Epoch 6/21\n",
      "118/118 [==============================] - 9s 76ms/step - loss: 0.0424 - get_f1: 0.9775\n",
      "Epoch 7/21\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 0.0294 - get_f1: 0.9850\n",
      "Epoch 8/21\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 0.0143 - get_f1: 0.9931\n",
      "Epoch 9/21\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 0.0089 - get_f1: 0.9967\n",
      "Epoch 10/21\n",
      "118/118 [==============================] - 9s 76ms/step - loss: 0.0056 - get_f1: 0.9982\n",
      "Epoch 11/21\n",
      "118/118 [==============================] - 9s 74ms/step - loss: 0.0037 - get_f1: 0.9992\n",
      "Epoch 12/21\n",
      "118/118 [==============================] - 9s 74ms/step - loss: 0.0024 - get_f1: 0.9996\n",
      "Epoch 13/21\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 0.0018 - get_f1: 0.9997\n",
      "Epoch 14/21\n",
      "118/118 [==============================] - 9s 74ms/step - loss: 0.0016 - get_f1: 0.9998\n",
      "Epoch 15/21\n",
      "118/118 [==============================] - 9s 76ms/step - loss: 0.0013 - get_f1: 0.9999\n",
      "Epoch 16/21\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 0.0010 - get_f1: 1.0000\n",
      "Epoch 17/21\n",
      "118/118 [==============================] - 9s 76ms/step - loss: 8.9226e-04 - get_f1: 1.0000\n",
      "Epoch 18/21\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 7.7985e-04 - get_f1: 1.0000\n",
      "Epoch 19/21\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 6.9026e-04 - get_f1: 1.0000\n",
      "Epoch 20/21\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 6.1685e-04 - get_f1: 1.0000\n",
      "Epoch 21/21\n",
      "118/118 [==============================] - 9s 75ms/step - loss: 5.5411e-04 - get_f1: 1.0000\n",
      "Senado - HAN tuned\n",
      "                                                                    precision    recall  f1-score   support\n",
      "\n",
      "                                                             Saúde      0.710     0.733     0.721        30\n",
      "                                               Relações Exteriores      0.986     0.966     0.976       294\n",
      "                                                     Meio ambiente      0.833     0.882     0.857        34\n",
      "                                       Educação, cultura e esporte      0.778     0.686     0.729        51\n",
      "                                        Segurança Pública e Defesa      0.869     0.894     0.882       104\n",
      "                                            Trabalho e Previdência      0.880     0.759     0.815        29\n",
      "                                     Agricultura, pecuária e pesca      0.850     0.773     0.810        44\n",
      "                                Ciência, tecnologia e comunicações      0.793     0.697     0.742        33\n",
      "                                                            Social      0.813     0.755     0.783        98\n",
      "Indústria, comércio, turismo, transporte/transporte de mercadorias      0.688     0.721     0.704        61\n",
      "                       Economia, planejamento e sistema financeiro      0.793     0.844     0.818       141\n",
      "                    Assuntos internos,  cargos e comissões, Estado      0.908     0.838     0.872       494\n",
      "                                                          Tributos      0.951     0.897     0.923        87\n",
      "                                                   Minas e Energia      0.850     0.895     0.872        38\n",
      "                                                Justiça e direitos      0.714     0.612     0.659        49\n",
      "\n",
      "                                                         micro avg      0.875     0.842     0.858      1587\n",
      "                                                         macro avg      0.828     0.797     0.811      1587\n",
      "                                                      weighted avg      0.876     0.842     0.858      1587\n",
      "                                                       samples avg      0.859     0.862     0.850      1587\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beta_2_real_p = [0.99,0.999,0.9999]\n",
    "\n",
    "Train_Y_sen = np.array([np.array(t) for t in Train_Y_sen])\n",
    "Test_Y_sen = np.array([np.array(t) for t in Test_Y_sen])\n",
    "\n",
    "model = HAN_model(len(Train_Y_sen[0]))\n",
    "\n",
    "opt = Adam(lr = parameters[\"learning_rate\"].iloc[0], beta_1 = parameters[\"beta1\"].iloc[0], beta_2 = beta_2_real_p[int(parameters[\"beta2\"].iloc[0])])\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1])\n",
    "history = model.fit(x=np.array(Train_X), y=np.array(Train_Y_sen), epochs=21, validation_split=0.0, class_weight=classes_w_sen, batch_size=45, shuffle=False)\n",
    "test_prediction = model.predict(np.array(Test_X))\n",
    "print(\"Senado - HAN tuned\")\n",
    "print(classification_report(y_true=np.array(Test_Y_sen), y_pred=predict_classes(test_prediction), digits=3, target_names=sen_classes_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/caiocampos/andre/radarenv/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/HAN_sen_model_final/assets\n"
     ]
    }
   ],
   "source": [
    "#Saving final model\n",
    "model.save('models/HAN_sen_model_final')\n",
    "pd.DataFrame(predict_classes(test_prediction)).to_csv(\"test_results/HAN_sen.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/15 classes: 0.0\n",
      "1/15 classes: 0.0\n",
      "2/15 classes: 0.0\n",
      "3/15 classes: 0.0\n",
      "4/15 classes: 0.0\n",
      "5/15 classes: 0.0\n",
      "6/15 classes: 0.0\n",
      "7/15 classes: 0.0\n",
      "8/15 classes: 0.0\n",
      "9/15 classes: 0.0\n",
      "10/15 classes: 0.0\n",
      "11/15 classes: 0.22371364653243847\n",
      "12/15 classes: 1.1185682326621924\n",
      "13/15 classes: 6.785980611483968\n",
      "14/15 classes: 15.06338553318419\n",
      "15/15 classes: 76.80835197613722\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(Test_Y_sen[0])+1):\n",
    "    print(str(i)+\"/\"+str(len(Test_Y_sen[0]))+\" classes:\", acerto_absoluto(Test_Y_sen, test_prediction, i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referenda - Otimização"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/caiocampos/andre/radar_wisemap/hypermapper\n",
      "Design of experiment phase, number of doe samples = 20 .......\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/caiocampos/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning:Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0:\n",
      "learning_rate:  0.061599517591355686  || beta1:  0.5106096601543278  || beta2:  0.9999  || epochs:  22  || batch_size:  42  || (1 - macro_F1):  1.0\n",
      "Iteration 1:\n",
      "learning_rate:  0.026570347917509094  || beta1:  0.6812362841760784  || beta2:  0.999  || epochs:  4  || batch_size:  45  || (1 - macro_F1):  0.5723655111136237\n",
      "Iteration 2:\n",
      "learning_rate:  0.09749756293549715  || beta1:  0.6620159003051941  || beta2:  0.99  || epochs:  12  || batch_size:  17  || (1 - macro_F1):  0.987342019035731\n",
      "Iteration 3:\n",
      "learning_rate:  0.03323013641474209  || beta1:  0.2641279302939091  || beta2:  0.9999  || epochs:  5  || batch_size:  30  || (1 - macro_F1):  0.9248318520750439\n",
      "Iteration 4:\n",
      "learning_rate:  0.027865443728216567  || beta1:  0.7524092317148126  || beta2:  0.99  || epochs:  19  || batch_size:  79  || (1 - macro_F1):  0.415843990611392\n",
      "Iteration 5:\n",
      "learning_rate:  0.09783954939500432  || beta1:  0.345983112285847  || beta2:  0.999  || epochs:  23  || batch_size:  34  || (1 - macro_F1):  1.0\n",
      "Iteration 6:\n",
      "learning_rate:  0.02634555450939013  || beta1:  0.4011240085444894  || beta2:  0.9999  || epochs:  7  || batch_size:  47  || (1 - macro_F1):  0.8038870995190253\n",
      "Iteration 7:\n",
      "learning_rate:  0.04814483538278692  || beta1:  0.8797556218497975  || beta2:  0.99  || epochs:  22  || batch_size:  57  || (1 - macro_F1):  0.8209331030848439\n",
      "Iteration 8:\n",
      "learning_rate:  0.09827735289388531  || beta1:  0.8646154950969777  || beta2:  0.999  || epochs:  22  || batch_size:  41  || (1 - macro_F1):  1.0\n",
      "Iteration 9:\n",
      "learning_rate:  0.04568435872534128  || beta1:  0.6531869176188992  || beta2:  0.9999  || epochs:  10  || batch_size:  46  || (1 - macro_F1):  1.0\n",
      "Iteration 10:\n",
      "learning_rate:  0.061915388871597574  || beta1:  0.6892117420750957  || beta2:  0.9999  || epochs:  25  || batch_size:  86  || (1 - macro_F1):  1.0\n",
      "Iteration 11:\n",
      "learning_rate:  0.04535900815866245  || beta1:  0.3467677567560928  || beta2:  0.99  || epochs:  10  || batch_size:  61  || (1 - macro_F1):  0.8619271803702644\n",
      "Iteration 12:\n",
      "learning_rate:  0.05456906202787509  || beta1:  0.4956547055107285  || beta2:  0.999  || epochs:  15  || batch_size:  54  || (1 - macro_F1):  1.0\n",
      "Iteration 13:\n",
      "learning_rate:  0.09690746568870338  || beta1:  0.7471866942680787  || beta2:  0.9999  || epochs:  17  || batch_size:  86  || (1 - macro_F1):  1.0\n",
      "Iteration 14:\n",
      "learning_rate:  0.033072644760170695  || beta1:  0.45587492950258623  || beta2:  0.9999  || epochs:  16  || batch_size:  52  || (1 - macro_F1):  0.9685732918557765\n",
      "Iteration 15:\n",
      "learning_rate:  0.007670640866931529  || beta1:  0.6295570136433658  || beta2:  0.999  || epochs:  29  || batch_size:  59  || (1 - macro_F1):  0.2621830627969436\n",
      "Iteration 16:\n",
      "learning_rate:  0.08440394255381535  || beta1:  0.720098770439258  || beta2:  0.9999  || epochs:  6  || batch_size:  57  || (1 - macro_F1):  0.9465190269763993\n",
      "Iteration 17:\n",
      "learning_rate:  0.07779711477371941  || beta1:  0.5126347653414803  || beta2:  0.99  || epochs:  24  || batch_size:  66  || (1 - macro_F1):  0.9656842106444351\n",
      "Iteration 18:\n",
      "learning_rate:  0.0418773981441932  || beta1:  0.6780384375920666  || beta2:  0.9999  || epochs:  27  || batch_size:  58  || (1 - macro_F1):  1.0\n",
      "Iteration 19:\n",
      "learning_rate:  0.0055  || beta1:  0.9  || beta2:  0.999  || epochs:  11  || batch_size:  55  || (1 - macro_F1):  0.24722089736569086\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.061599517591355686,0.5106096601543278,2,22,42,1.0,564290\n",
      "0.026570347917509094,0.6812362841760784,1,4,45,0.5723655111136237,696861\n",
      "0.09749756293549715,0.6620159003051941,0,12,17,0.987342019035731,1120359\n",
      "0.03323013641474209,0.2641279302939091,2,5,30,0.9248318520750439,1289356\n",
      "0.027865443728216567,0.7524092317148126,0,19,79,0.415843990611392,1731189\n",
      "0.09783954939500432,0.345983112285847,1,23,34,1.0,2345866\n",
      "0.02634555450939013,0.4011240085444894,2,7,47,0.8038870995190253,2546510\n",
      "0.04814483538278692,0.8797556218497975,0,22,57,0.8209331030848439,3079775\n",
      "0.09827735289388531,0.8646154950969777,1,22,41,1.0,3647492\n",
      "0.04568435872534128,0.6531869176188992,2,10,46,1.0,3919073\n",
      "0.061915388871597574,0.6892117420750957,2,25,86,1.0,4481414\n",
      "0.04535900815866245,0.3467677567560928,0,10,61,0.8619271803702644,4742077\n",
      "0.05456906202787509,0.4956547055107285,1,15,54,1.0,5121603\n",
      "0.09690746568870338,0.7471866942680787,2,17,86,1.0,5514093\n",
      "0.033072644760170695,0.45587492950258623,2,16,52,0.9685732918557765,5918390\n",
      "0.007670640866931529,0.6295570136433658,1,29,59,0.2621830627969436,6604816\n",
      "0.08440394255381535,0.720098770439258,2,6,57,0.9465190269763993,6778554\n",
      "0.07779711477371941,0.5126347653414803,0,24,66,0.9656842106444351,7340955\n",
      "0.0418773981441932,0.6780384375920666,2,27,58,1.0,7984056\n",
      "0.0055,0.9,1,11,55,0.24722089736569086,8272215\n",
      "\n",
      "\n",
      "End of doe phase, the number of new configuration runs is: 20\n",
      "\n",
      "Starting optimization iteration 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/caiocampos/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning:Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 20:\n",
      "learning_rate:  0.08662981691696064  || beta1:  0.65672438300679  || beta2:  0.9999  || epochs:  11  || batch_size:  50  || (1 - macro_F1):  1.0\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.08662981691696064,0.65672438300679,2,11,50,1.0,8569655\n",
      "\n",
      "Starting optimization iteration 2\n",
      "Iteration 21:\n",
      "learning_rate:  0.005433563891625506  || beta1:  0.778660186261144  || beta2:  0.9999  || epochs:  11  || batch_size:  61  || (1 - macro_F1):  0.24094542102372973\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.005433563891625506,0.778660186261144,2,11,61,0.24094542102372973,8859850\n",
      "\n",
      "Starting optimization iteration 3\n",
      "Iteration 22:\n",
      "learning_rate:  0.008381103015120234  || beta1:  0.4251389280472371  || beta2:  0.999  || epochs:  11  || batch_size:  57  || (1 - macro_F1):  0.24946354750518607\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.008381103015120234,0.4251389280472371,1,11,57,0.24946354750518607,9153180\n",
      "\n",
      "Starting optimization iteration 4\n",
      "Iteration 23:\n",
      "learning_rate:  0.008381103015120234  || beta1:  0.9  || beta2:  0.999  || epochs:  11  || batch_size:  56  || (1 - macro_F1):  0.2503364566293892\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.008381103015120234,0.9,1,11,56,0.2503364566293892,9446212\n",
      "\n",
      "Starting optimization iteration 5\n",
      "Iteration 24:\n",
      "learning_rate:  0.005433563891625506  || beta1:  0.9  || beta2:  0.9999  || epochs:  11  || batch_size:  60  || (1 - macro_F1):  0.2619031918524457\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.005433563891625506,0.9,2,11,60,0.2619031918524457,9735588\n",
      "\n",
      "Starting optimization iteration 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/caiocampos/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning:Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 25:\n",
      "learning_rate:  8.969365398429023e-05  || beta1:  0.45217973681160784  || beta2:  0.999  || epochs:  3  || batch_size:  28  || (1 - macro_F1):  1.0\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "8.969365398429023e-05,0.45217973681160784,1,3,28,1.0,9861582\n",
      "\n",
      "Starting optimization iteration 7\n",
      "Iteration 26:\n",
      "learning_rate:  0.0006670355079480684  || beta1:  0.9  || beta2:  0.9999  || epochs:  11  || batch_size:  66  || (1 - macro_F1):  0.3030072448946455\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0006670355079480684,0.9,2,11,66,0.3030072448946455,10146154\n",
      "\n",
      "Starting optimization iteration 8\n",
      "Iteration 27:\n",
      "learning_rate:  0.004618960314542974  || beta1:  0.255781418146325  || beta2:  0.999  || epochs:  11  || batch_size:  62  || (1 - macro_F1):  0.24953025434245757\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.004618960314542974,0.255781418146325,1,11,62,0.24953025434245757,10433955\n",
      "\n",
      "Starting optimization iteration 9\n",
      "Iteration 28:\n",
      "learning_rate:  0.003689569788454577  || beta1:  0.24179692595693225  || beta2:  0.9999  || epochs:  5  || batch_size:  62  || (1 - macro_F1):  0.26645583093319336\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.003689569788454577,0.24179692595693225,2,5,62,0.26645583093319336,10589240\n",
      "\n",
      "Starting optimization iteration 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/caiocampos/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning:Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 29:\n",
      "learning_rate:  0.010917205730557145  || beta1:  0.9  || beta2:  0.9999  || epochs:  3  || batch_size:  71  || (1 - macro_F1):  0.3532341735150063\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.010917205730557145,0.9,2,3,71,0.3532341735150063,10699201\n",
      "\n",
      "Starting optimization iteration 11\n",
      "Iteration 30:\n",
      "learning_rate:  0.007233585249118387  || beta1:  0.9  || beta2:  0.9999  || epochs:  22  || batch_size:  64  || (1 - macro_F1):  0.26540456522822875\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.007233585249118387,0.9,2,22,64,0.26540456522822875,11228716\n",
      "\n",
      "Starting optimization iteration 12\n",
      "Iteration 31:\n",
      "learning_rate:  0.0062180560936290034  || beta1:  0.9  || beta2:  0.99  || epochs:  11  || batch_size:  61  || (1 - macro_F1):  0.2626805227630168\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0062180560936290034,0.9,0,11,61,0.2626805227630168,11517307\n",
      "\n",
      "Starting optimization iteration 13\n",
      "Iteration 32:\n",
      "learning_rate:  0.009433673931947809  || beta1:  0.4066568778957826  || beta2:  0.9999  || epochs:  11  || batch_size:  68  || (1 - macro_F1):  0.24341593087250235\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.009433673931947809,0.4066568778957826,2,11,68,0.24341593087250235,11800323\n",
      "\n",
      "Starting optimization iteration 14\n",
      "Iteration 33:\n",
      "learning_rate:  0.020098204661895026  || beta1:  0.14905066093015104  || beta2:  0.9999  || epochs:  11  || batch_size:  66  || (1 - macro_F1):  0.342967902047814\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.020098204661895026,0.14905066093015104,2,11,66,0.342967902047814,12083153\n",
      "\n",
      "Starting optimization iteration 15\n",
      "Iteration 34:\n",
      "learning_rate:  0.00894029657110117  || beta1:  0.9  || beta2:  0.9999  || epochs:  11  || batch_size:  64  || (1 - macro_F1):  0.2584724944770307\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.00894029657110117,0.9,2,11,64,0.2584724944770307,12368182\n",
      "\n",
      "Starting optimization iteration 16\n",
      "Iteration 35:\n",
      "learning_rate:  0.005436596891669702  || beta1:  0.10278102795811428  || beta2:  0.9999  || epochs:  15  || batch_size:  63  || (1 - macro_F1):  0.24758659236054092\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.005436596891669702,0.10278102795811428,2,15,63,0.24758659236054092,12742653\n",
      "\n",
      "Starting optimization iteration 17\n",
      "Iteration 36:\n",
      "learning_rate:  0.010419587076365186  || beta1:  0.03160591604347548  || beta2:  0.9999  || epochs:  11  || batch_size:  70  || (1 - macro_F1):  0.24432457031011456\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.010419587076365186,0.03160591604347548,2,11,70,0.24432457031011456,13026012\n",
      "\n",
      "Starting optimization iteration 18\n",
      "Iteration 37:\n",
      "learning_rate:  0.005767321740697685  || beta1:  0.255781418146325  || beta2:  0.9999  || epochs:  11  || batch_size:  62  || (1 - macro_F1):  0.24712804066193794\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.005767321740697685,0.255781418146325,2,11,62,0.24712804066193794,13313020\n",
      "\n",
      "Starting optimization iteration 19\n",
      "Iteration 38:\n",
      "learning_rate:  0.011548399883527639  || beta1:  0.9  || beta2:  0.9999  || epochs:  21  || batch_size:  70  || (1 - macro_F1):  0.26172699215616535\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.011548399883527639,0.9,2,21,70,0.26172699215616535,13811767\n",
      "\n",
      "Starting optimization iteration 20\n",
      "Iteration 39:\n",
      "learning_rate:  0.0072251947046114645  || beta1:  0.778660186261144  || beta2:  0.999  || epochs:  17  || batch_size:  60  || (1 - macro_F1):  0.253680991692238\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0072251947046114645,0.778660186261144,1,17,60,0.253680991692238,14234137\n",
      "\n",
      "Starting optimization iteration 21\n",
      "Iteration 40:\n",
      "learning_rate:  0.00960207309345031  || beta1:  0.09746555312715278  || beta2:  0.9999  || epochs:  17  || batch_size:  68  || (1 - macro_F1):  0.25136408354532425\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.00960207309345031,0.09746555312715278,2,17,68,0.25136408354532425,14649186\n",
      "\n",
      "Starting optimization iteration 22\n",
      "Iteration 41:\n",
      "learning_rate:  0.003050312788267927  || beta1:  0.255781418146325  || beta2:  0.999  || epochs:  12  || batch_size:  58  || (1 - macro_F1):  0.2548273140772448\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.003050312788267927,0.255781418146325,1,12,58,0.2548273140772448,14962054\n",
      "\n",
      "Starting optimization iteration 23\n",
      "Iteration 42:\n",
      "learning_rate:  0.003393664936908614  || beta1:  0.5627418664569723  || beta2:  0.999  || epochs:  17  || batch_size:  64  || (1 - macro_F1):  0.256273788613514\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.003393664936908614,0.5627418664569723,1,17,64,0.256273788613514,15378890\n",
      "\n",
      "Starting optimization iteration 24\n",
      "Iteration 43:\n",
      "learning_rate:  0.01013486445325809  || beta1:  0.09746555312715278  || beta2:  0.999  || epochs:  19  || batch_size:  68  || (1 - macro_F1):  0.25316182533503473\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.01013486445325809,0.09746555312715278,1,19,68,0.25316182533503473,15836529\n",
      "\n",
      "Starting optimization iteration 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/caiocampos/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning:Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 44:\n",
      "learning_rate:  4.576726164470858e-05  || beta1:  0.9  || beta2:  0.999  || epochs:  29  || batch_size:  56  || (1 - macro_F1):  0.8756976319331538\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "4.576726164470858e-05,0.9,1,29,56,0.8756976319331538,16533806\n",
      "\n",
      "Starting optimization iteration 26\n",
      "Iteration 45:\n",
      "learning_rate:  0.002501503699863985  || beta1:  0.6400557310921192  || beta2:  0.9999  || epochs:  3  || batch_size:  56  || (1 - macro_F1):  0.3050038083898542\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.002501503699863985,0.6400557310921192,2,3,56,0.3050038083898542,16646146\n",
      "\n",
      "Starting optimization iteration 27\n",
      "Iteration 46:\n",
      "learning_rate:  0.010838044993777494  || beta1:  0.8846383503811964  || beta2:  0.9999  || epochs:  28  || batch_size:  60  || (1 - macro_F1):  0.26014692303000475\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.010838044993777494,0.8846383503811964,2,28,60,0.26014692303000475,17314301\n",
      "\n",
      "Starting optimization iteration 28\n",
      "Iteration 47:\n",
      "learning_rate:  0.005878573757737465  || beta1:  0.9  || beta2:  0.9999  || epochs:  11  || batch_size:  56  || (1 - macro_F1):  0.2510163833575151\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.005878573757737465,0.9,2,11,56,0.2510163833575151,17606904\n",
      "\n",
      "Starting optimization iteration 29\n",
      "Iteration 48:\n",
      "learning_rate:  0.009840005201081126  || beta1:  0.4703103533236078  || beta2:  0.999  || epochs:  27  || batch_size:  64  || (1 - macro_F1):  0.2634677249764693\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.009840005201081126,0.4703103533236078,1,27,64,0.2634677249764693,18244264\n",
      "\n",
      "Starting optimization iteration 30\n",
      "Iteration 49:\n",
      "learning_rate:  0.012841671771691005  || beta1:  0.9  || beta2:  0.9999  || epochs:  13  || batch_size:  67  || (1 - macro_F1):  0.2539632105445442\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.012841671771691005,0.9,2,13,67,0.2539632105445442,18573507\n",
      "\n",
      "Starting optimization iteration 31\n",
      "Iteration 50:\n",
      "learning_rate:  0.0007290756127029903  || beta1:  0.26612974206525447  || beta2:  0.999  || epochs:  29  || batch_size:  61  || (1 - macro_F1):  0.2964922485045185\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0007290756127029903,0.26612974206525447,1,29,61,0.2964922485045185,19260735\n",
      "\n",
      "Starting optimization iteration 32\n",
      "Iteration 51:\n",
      "learning_rate:  0.00765651055589453  || beta1:  0.03564773418984018  || beta2:  0.999  || epochs:  29  || batch_size:  61  || (1 - macro_F1):  0.34718829219158476\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.00765651055589453,0.03564773418984018,1,29,61,0.34718829219158476,19947779\n",
      "\n",
      "Starting optimization iteration 33\n",
      "Iteration 52:\n",
      "learning_rate:  0.011549833070445648  || beta1:  0.6300825526909786  || beta2:  0.9999  || epochs:  23  || batch_size:  58  || (1 - macro_F1):  0.2487467763184541\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.011549833070445648,0.6300825526909786,2,23,58,0.2487467763184541,20506386\n",
      "\n",
      "Starting optimization iteration 34\n",
      "Iteration 53:\n",
      "learning_rate:  0.011549833070445648  || beta1:  0.9  || beta2:  0.9999  || epochs:  24  || batch_size:  59  || (1 - macro_F1):  0.2575082451728763\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.011549833070445648,0.9,2,24,59,0.2575082451728763,21087051\n",
      "\n",
      "Starting optimization iteration 35\n",
      "Iteration 54:\n",
      "learning_rate:  0.00012526790008398245  || beta1:  0.9  || beta2:  0.999  || epochs:  29  || batch_size:  66  || (1 - macro_F1):  0.44994111677095305\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.00012526790008398245,0.9,1,29,66,0.44994111677095305,21763237\n",
      "\n",
      "Starting optimization iteration 36\n",
      "Iteration 55:\n",
      "learning_rate:  0.010702235469739554  || beta1:  0.9  || beta2:  0.999  || epochs:  25  || batch_size:  65  || (1 - macro_F1):  0.2644955884164317\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.010702235469739554,0.9,1,25,65,0.2644955884164317,22355151\n",
      "\n",
      "Starting optimization iteration 37\n",
      "Iteration 56:\n",
      "learning_rate:  0.011549833070445648  || beta1:  0.9  || beta2:  0.999  || epochs:  28  || batch_size:  58  || (1 - macro_F1):  0.3026357219958732\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.011549833070445648,0.9,1,28,58,0.3026357219958732,23027295\n",
      "\n",
      "Starting optimization iteration 38\n",
      "Iteration 57:\n",
      "learning_rate:  0.0015616503819665373  || beta1:  0.5304392573244316  || beta2:  0.99  || epochs:  14  || batch_size:  62  || (1 - macro_F1):  0.28307601350422584\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0015616503819665373,0.5304392573244316,0,14,62,0.28307601350422584,23379506\n",
      "\n",
      "Starting optimization iteration 39\n",
      "Iteration 58:\n",
      "learning_rate:  0.002375001426333729  || beta1:  0.9  || beta2:  0.999  || epochs:  25  || batch_size:  62  || (1 - macro_F1):  0.26714007454192745\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.002375001426333729,0.9,1,25,62,0.26714007454192745,23976948\n",
      "\n",
      "Starting optimization iteration 40\n",
      "Iteration 59:\n",
      "learning_rate:  0.007545436654892996  || beta1:  0.9  || beta2:  0.9999  || epochs:  27  || batch_size:  62  || (1 - macro_F1):  0.2686344366057587\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.007545436654892996,0.9,2,27,62,0.2686344366057587,24616596\n",
      "\n",
      "Starting optimization iteration 41\n",
      "Iteration 60:\n",
      "learning_rate:  0.0068578246573808704  || beta1:  0.10419865900074508  || beta2:  0.9999  || epochs:  17  || batch_size:  58  || (1 - macro_F1):  0.24473104838473114\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0068578246573808704,0.10419865900074508,2,17,58,0.24473104838473114,25042136\n",
      "\n",
      "Starting optimization iteration 42\n",
      "Iteration 61:\n",
      "learning_rate:  0.00331181868799881  || beta1:  0.9  || beta2:  0.999  || epochs:  29  || batch_size:  60  || (1 - macro_F1):  0.2696623912755205\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.00331181868799881,0.9,1,29,60,0.2696623912755205,25731921\n",
      "\n",
      "Starting optimization iteration 43\n",
      "Iteration 62:\n",
      "learning_rate:  0.005433563891625506  || beta1:  0.778660186261144  || beta2:  0.99  || epochs:  12  || batch_size:  61  || (1 - macro_F1):  0.25033449759115256\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.005433563891625506,0.778660186261144,0,12,61,0.25033449759115256,26041996\n",
      "\n",
      "Starting optimization iteration 44\n",
      "Iteration 63:\n",
      "learning_rate:  0.006048249766919956  || beta1:  0.11605185793526884  || beta2:  0.99  || epochs:  12  || batch_size:  60  || (1 - macro_F1):  0.2560708714773441\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.006048249766919956,0.11605185793526884,0,12,60,0.2560708714773441,26354763\n",
      "\n",
      "Starting optimization iteration 45\n",
      "Iteration 64:\n",
      "learning_rate:  0.006545418929555008  || beta1:  0.765027168603314  || beta2:  0.999  || epochs:  28  || batch_size:  59  || (1 - macro_F1):  0.25757119334805867\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.006545418929555008,0.765027168603314,1,28,59,0.25757119334805867,27025142\n",
      "\n",
      "Starting optimization iteration 46\n",
      "Iteration 65:\n",
      "learning_rate:  0.005605218527809751  || beta1:  0.7432058182465886  || beta2:  0.99  || epochs:  14  || batch_size:  59  || (1 - macro_F1):  0.2586959449830708\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.005605218527809751,0.7432058182465886,0,14,59,0.2586959449830708,27382257\n",
      "\n",
      "Starting optimization iteration 47\n",
      "Iteration 66:\n",
      "learning_rate:  0.0032435103747275274  || beta1:  0.2913279601397585  || beta2:  0.9999  || epochs:  15  || batch_size:  68  || (1 - macro_F1):  0.25368805063644806\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0032435103747275274,0.2913279601397585,2,15,68,0.25368805063644806,27753385\n",
      "\n",
      "Starting optimization iteration 48\n",
      "Iteration 67:\n",
      "learning_rate:  0.0005844216603052998  || beta1:  0.7077179515209419  || beta2:  0.99  || epochs:  7  || batch_size:  59  || (1 - macro_F1):  0.4081493890756115\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0005844216603052998,0.7077179515209419,0,7,59,0.4081493890756115,27953963\n",
      "\n",
      "Starting optimization iteration 49\n",
      "Iteration 68:\n",
      "learning_rate:  0.007962584760087394  || beta1:  0.7353795361045266  || beta2:  0.999  || epochs:  23  || batch_size:  62  || (1 - macro_F1):  0.25704623655435155\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.007962584760087394,0.7353795361045266,1,23,62,0.25704623655435155,28507090\n",
      "\n",
      "Starting optimization iteration 50\n",
      "Iteration 69:\n",
      "learning_rate:  0.0032435103747275274  || beta1:  0.10638979118883961  || beta2:  0.9999  || epochs:  16  || batch_size:  66  || (1 - macro_F1):  0.2584281783343172\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0032435103747275274,0.10638979118883961,2,16,66,0.2584281783343172,28901272\n",
      "\n",
      "Starting optimization iteration 51\n",
      "Iteration 70:\n",
      "learning_rate:  0.0055  || beta1:  0.9  || beta2:  0.99  || epochs:  20  || batch_size:  57  || (1 - macro_F1):  0.25336778555535466\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0055,0.9,0,20,57,0.25336778555535466,29395565\n",
      "\n",
      "Starting optimization iteration 52\n",
      "Iteration 71:\n",
      "learning_rate:  0.005820957138226527  || beta1:  0.7214772943107742  || beta2:  0.99  || epochs:  22  || batch_size:  60  || (1 - macro_F1):  0.25791770039869955\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.005820957138226527,0.7214772943107742,0,22,60,0.25791770039869955,29929987\n",
      "\n",
      "Starting optimization iteration 53\n",
      "Iteration 72:\n",
      "learning_rate:  0.0030671832377956094  || beta1:  0.1911619422844742  || beta2:  0.9999  || epochs:  19  || batch_size:  60  || (1 - macro_F1):  0.25807198088845495\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0030671832377956094,0.1911619422844742,2,19,60,0.25807198088845495,30397081\n",
      "\n",
      "Starting optimization iteration 54\n",
      "Iteration 73:\n",
      "learning_rate:  0.008381103015120234  || beta1:  0.4251389280472371  || beta2:  0.999  || epochs:  13  || batch_size:  69  || (1 - macro_F1):  0.24957690115287945\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.008381103015120234,0.4251389280472371,1,13,69,0.24957690115287945,30722771\n",
      "\n",
      "Starting optimization iteration 55\n",
      "Iteration 74:\n",
      "learning_rate:  0.0006445484508916296  || beta1:  0.6320746414025864  || beta2:  0.99  || epochs:  22  || batch_size:  62  || (1 - macro_F1):  0.3003129770182348\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0006445484508916296,0.6320746414025864,0,22,62,0.3003129770182348,31252054\n",
      "\n",
      "Starting optimization iteration 56\n",
      "Iteration 75:\n",
      "learning_rate:  0.006737799525273353  || beta1:  0.9  || beta2:  0.9999  || epochs:  23  || batch_size:  59  || (1 - macro_F1):  0.2625885101883072\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.006737799525273353,0.9,2,23,59,0.2625885101883072,31810884\n",
      "\n",
      "Starting optimization iteration 57\n",
      "Iteration 76:\n",
      "learning_rate:  0.0016744837344242493  || beta1:  0.7438835770853172  || beta2:  0.99  || epochs:  18  || batch_size:  61  || (1 - macro_F1):  0.2655013067037968\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0016744837344242493,0.7438835770853172,0,18,61,0.2655013067037968,32253352\n",
      "\n",
      "Starting optimization iteration 58\n",
      "Iteration 77:\n",
      "learning_rate:  0.005324190697035055  || beta1:  0.5396729780645276  || beta2:  0.99  || epochs:  11  || batch_size:  83  || (1 - macro_F1):  0.26624403939638086\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.005324190697035055,0.5396729780645276,0,11,83,0.26624403939638086,32529596\n",
      "\n",
      "Starting optimization iteration 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/caiocampos/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning:Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 78:\n",
      "learning_rate:  0.034888230852272564  || beta1:  0.5996127255906881  || beta2:  0.9999  || epochs:  20  || batch_size:  64  || (1 - macro_F1):  1.0\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.034888230852272564,0.5996127255906881,2,20,64,1.0,33012797\n",
      "\n",
      "Starting optimization iteration 60\n",
      "Iteration 79:\n",
      "learning_rate:  0.0013003431703968867  || beta1:  0.42407623307251663  || beta2:  0.99  || epochs:  11  || batch_size:  94  || (1 - macro_F1):  0.29255636774174065\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0013003431703968867,0.42407623307251663,0,11,94,0.29255636774174065,33284715\n",
      "\n",
      "Starting optimization iteration 61\n",
      "Iteration 80:\n",
      "learning_rate:  0.0030512337386958705  || beta1:  0.9  || beta2:  0.99  || epochs:  20  || batch_size:  59  || (1 - macro_F1):  0.26686955915232646\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0030512337386958705,0.9,0,20,59,0.26686955915232646,33777234\n",
      "\n",
      "Starting optimization iteration 62\n",
      "Iteration 81:\n",
      "learning_rate:  0.010419587076365186  || beta1:  0.07147633248140542  || beta2:  0.99  || epochs:  16  || batch_size:  70  || (1 - macro_F1):  0.2684341713088587\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.010419587076365186,0.07147633248140542,0,16,70,0.2684341713088587,34168151\n",
      "\n",
      "Starting optimization iteration 63\n",
      "Iteration 82:\n",
      "learning_rate:  0.01306004414481156  || beta1:  0.3746770617838208  || beta2:  0.9999  || epochs:  12  || batch_size:  63  || (1 - macro_F1):  0.24475852837283651\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.01306004414481156,0.3746770617838208,2,12,63,0.24475852837283651,34476867\n",
      "\n",
      "Starting optimization iteration 64\n",
      "Iteration 83:\n",
      "learning_rate:  0.017266931914545137  || beta1:  0.03160591604347548  || beta2:  0.9999  || epochs:  29  || batch_size:  94  || (1 - macro_F1):  0.2692273033700263\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.017266931914545137,0.03160591604347548,2,29,94,0.2692273033700263,35122087\n",
      "\n",
      "Starting optimization iteration 65\n",
      "Iteration 84:\n",
      "learning_rate:  0.006425961109230755  || beta1:  0.031814286586332646  || beta2:  0.9999  || epochs:  3  || batch_size:  68  || (1 - macro_F1):  0.2829780549671005\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.006425961109230755,0.031814286586332646,2,3,68,0.2829780549671005,35233149\n",
      "\n",
      "Starting optimization iteration 66\n",
      "Iteration 85:\n",
      "learning_rate:  0.005941510217969125  || beta1:  0.2888385528642119  || beta2:  0.9999  || epochs:  9  || batch_size:  54  || (1 - macro_F1):  0.24586299434254322\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.005941510217969125,0.2888385528642119,2,9,54,0.24586299434254322,35483081\n",
      "\n",
      "Starting optimization iteration 67\n",
      "Iteration 86:\n",
      "learning_rate:  0.015020319374618446  || beta1:  0.41319337540239115  || beta2:  0.9999  || epochs:  17  || batch_size:  69  || (1 - macro_F1):  0.24786156842787177\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.015020319374618446,0.41319337540239115,2,17,69,0.24786156842787177,35895726\n",
      "\n",
      "Starting optimization iteration 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/caiocampos/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning:Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 87:\n",
      "learning_rate:  0.029684467830507072  || beta1:  0.33018821068313015  || beta2:  0.9999  || epochs:  3  || batch_size:  100  || (1 - macro_F1):  0.855123296957921\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.029684467830507072,0.33018821068313015,2,3,100,0.855123296957921,36003164\n",
      "\n",
      "Starting optimization iteration 69\n",
      "Iteration 88:\n",
      "learning_rate:  0.01071889744900273  || beta1:  0.7219145785873757  || beta2:  0.9999  || epochs:  29  || batch_size:  90  || (1 - macro_F1):  0.29445245454919555\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.01071889744900273,0.7219145785873757,2,29,90,0.29445245454919555,36650484\n",
      "\n",
      "Starting optimization iteration 70\n",
      "Iteration 89:\n",
      "learning_rate:  0.006067389137019261  || beta1:  0.8168720133595471  || beta2:  0.999  || epochs:  3  || batch_size:  54  || (1 - macro_F1):  0.3031124294830416\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.006067389137019261,0.8168720133595471,1,3,54,0.3031124294830416,36765444\n",
      "\n",
      "Starting optimization iteration 71\n",
      "Iteration 90:\n",
      "learning_rate:  0.011794378909798744  || beta1:  0.9  || beta2:  0.9999  || epochs:  19  || batch_size:  62  || (1 - macro_F1):  0.26065407963541665\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.011794378909798744,0.9,2,19,62,0.26065407963541665,37230589\n",
      "\n",
      "Starting optimization iteration 72\n",
      "Iteration 91:\n",
      "learning_rate:  0.009175406219814553  || beta1:  0.778660186261144  || beta2:  0.9999  || epochs:  29  || batch_size:  100  || (1 - macro_F1):  0.26664669052089396\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.009175406219814553,0.778660186261144,2,29,100,0.26664669052089396,37872327\n",
      "\n",
      "Starting optimization iteration 73\n",
      "Iteration 92:\n",
      "learning_rate:  0.007956372205865881  || beta1:  0.9  || beta2:  0.9999  || epochs:  29  || batch_size:  53  || (1 - macro_F1):  0.26020921545722686\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.007956372205865881,0.9,2,29,53,0.26020921545722686,38579289\n",
      "\n",
      "Starting optimization iteration 74\n",
      "Iteration 93:\n",
      "learning_rate:  0.0013743519880700318  || beta1:  0.4446954313715811  || beta2:  0.9999  || epochs:  29  || batch_size:  78  || (1 - macro_F1):  0.27869695941794936\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.0013743519880700318,0.4446954313715811,2,29,78,0.27869695941794936,39238520\n",
      "\n",
      "Starting optimization iteration 75\n",
      "Iteration 94:\n",
      "learning_rate:  0.008056657800281294  || beta1:  0.9  || beta2:  0.9999  || epochs:  20  || batch_size:  55  || (1 - macro_F1):  0.26078763935969795\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.008056657800281294,0.9,2,20,55,0.26078763935969795,39735755\n",
      "\n",
      "Starting optimization iteration 76\n",
      "Iteration 95:\n",
      "learning_rate:  0.001993107264750502  || beta1:  0.8849083080398366  || beta2:  0.9999  || epochs:  17  || batch_size:  78  || (1 - macro_F1):  0.2617020711179119\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.001993107264750502,0.8849083080398366,2,17,78,0.2617020711179119,40140406\n",
      "\n",
      "Starting optimization iteration 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/caiocampos/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning:Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 96:\n",
      "learning_rate:  0.06437711313136385  || beta1:  0.9  || beta2:  0.999  || epochs:  29  || batch_size:  16  || (1 - macro_F1):  1.0\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.06437711313136385,0.9,1,29,16,1.0,41133982\n",
      "\n",
      "Starting optimization iteration 78\n",
      "Iteration 97:\n",
      "learning_rate:  0.00023338506981298714  || beta1:  0.513240005077079  || beta2:  0.9999  || epochs:  29  || batch_size:  86  || (1 - macro_F1):  0.3310668325559112\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.00023338506981298714,0.513240005077079,2,29,86,0.3310668325559112,41786049\n",
      "\n",
      "Starting optimization iteration 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /home/caiocampos/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning:Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 98:\n",
      "learning_rate:  0.020668523640199374  || beta1:  0.9  || beta2:  0.9999  || epochs:  26  || batch_size:  78  || (1 - macro_F1):  0.591029848467981\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.020668523640199374,0.9,2,26,78,0.591029848467981,42385231\n",
      "\n",
      "Starting optimization iteration 80\n",
      "Iteration 99:\n",
      "learning_rate:  0.011060078644466382  || beta1:  0.23512768872592774  || beta2:  0.9999  || epochs:  3  || batch_size:  95  || (1 - macro_F1):  0.28678990215577094\n",
      "learning_rate,beta1,beta2,epochs,batch_size,1 - F1,Timestamp\n",
      "0.011060078644466382,0.23512768872592774,2,3,95,0.28678990215577094,42491655\n",
      "\n",
      "End of Random Scalarizations\n",
      "### End of the hypermapper script.\n"
     ]
    }
   ],
   "source": [
    "stdout = sys.stdout\n",
    "os.chdir(\"/home/caiocampos/andre/radar_wisemap\")\n",
    "sys.path.append('hypermapper/scripts/')\n",
    "import hypermapper\n",
    "\n",
    "iterations = 0\n",
    "\n",
    "def optimize_HAN_minist(parameters):\n",
    "    global iterations\n",
    "    f1s = 0\n",
    "    f1s_per_class = []\n",
    "    precision_per_class = []\n",
    "    recall_per_class = []\n",
    "    beta_2_real_p = [0.99,0.999,0.9999]  # O parâmetro beta2 é traduzido de categórico para um dos três reais descritos no paper do Adam.\n",
    "    \n",
    "    for i in range(len(train_sets)):        \n",
    "        sets_index = []\n",
    "        for t in range(len(train_sets)):\n",
    "            if t!=i:\n",
    "                sets_index.append(t)\n",
    "\n",
    "        train_X = np.concatenate(np.array(sets_X, dtype=object)[sets_index])\n",
    "        validation_X = sets_X[i]\n",
    "\n",
    "        train_Y_minist = np.concatenate(np.array(sets_Y_minist, dtype=object)[sets_index])\n",
    "        train_Y_minist = np.array([np.array(t) for t in train_Y_minist])\n",
    "        validation_Y_minist = sets_Y_minist[i]\n",
    "        validation_Y_minist = np.array([np.array(v) for v in validation_Y_minist])\n",
    "\n",
    "        model = HAN_model(len(Train_Y_minist[0]))\n",
    "        opt = Adam(lr=parameters['learning_rate'], beta_1=parameters['beta1'], beta_2=beta_2_real_p[parameters['beta2']])\n",
    "        model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1])\n",
    "        history = model.fit(x=np.array(train_X), y=np.array(train_Y_minist), epochs=parameters['epochs'], validation_split=0.0, class_weight=classes_w_minist, batch_size=parameters['batch_size'], shuffle=False, verbose=0)\n",
    "        validation_prediction = model.predict(np.array(validation_X))\n",
    "\n",
    "        f1s+=f1_score(y_true=validation_Y_minist, y_pred=predict_classes(validation_prediction), average='macro')\n",
    "        f1s_per_class.append(f1_score(y_true=validation_Y_minist, y_pred=predict_classes(validation_prediction), average=None))\n",
    "        precision_per_class.append(precision_score(y_true=validation_Y_minist, y_pred=predict_classes(validation_prediction), average=None))\n",
    "        recall_per_class.append(recall_score(y_true=validation_Y_minist, y_pred=predict_classes(validation_prediction), average=None))\n",
    "    generate_validation_csv(f1s_per_class, precision_per_class, recall_per_class, \"HAN_minist_v2\", minist_classes_names)\n",
    "    f1_loss=1-f1s/4\n",
    "    print(\"Iteration \"+str(iterations)+\":\\nlearning_rate: \", parameters['learning_rate'], \" || beta1: \", parameters['beta1'], \" || beta2: \", beta_2_real_p[parameters['beta2']], \" || epochs: \", parameters['epochs'], \" || batch_size: \", parameters['batch_size'], \" || (1 - macro_F1): \", f1_loss)\n",
    "    iterations+=1\n",
    "    return f1_loss\n",
    "\n",
    "os.chdir(\"hypermapper/\")\n",
    "stdout = sys.stdout\n",
    "print(os.getcwd())\n",
    "hypermapper.optimize(\"../optimization/HAN_scenario.json\", optimize_HAN_minist)\n",
    "os.chdir(\"/home/caiocampos/andre/radar_wisemap/\")\n",
    "sys.stdout = stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>beta1</th>\n",
       "      <th>beta2</th>\n",
       "      <th>epochs</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>1 - F1</th>\n",
       "      <th>Timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.005434</td>\n",
       "      <td>0.77866</td>\n",
       "      <td>2.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0.240945</td>\n",
       "      <td>8859850.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    learning_rate    beta1  beta2  epochs  batch_size    1 - F1  Timestamp\n",
       "21       0.005434  0.77866    2.0    11.0        61.0  0.240945  8859850.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimization_results_minist = pd.read_csv(\"optimization/minist_han_validation_hypermapper_output_v2.csv\")\n",
    "best_parameters_idx_minist = optimization_results_minist[\"1 - F1\"].idxmin()\n",
    "parameters_minist = pd.DataFrame(optimization_results_minist.iloc[best_parameters_idx_minist]).transpose()\n",
    "parameters_minist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Saúde</th>\n",
       "      <th>Relações Exteriores</th>\n",
       "      <th>Meio ambiente</th>\n",
       "      <th>Educação, cultura e esporte</th>\n",
       "      <th>Justiça e Segurança</th>\n",
       "      <th>Trabalho e Previdência</th>\n",
       "      <th>Transporte/transporte de mercadorias</th>\n",
       "      <th>Agricultura, pecuária e pesca</th>\n",
       "      <th>Ciência e tecnologia</th>\n",
       "      <th>Social</th>\n",
       "      <th>Presidência</th>\n",
       "      <th>Economia e planejamento</th>\n",
       "      <th>Indústria, comércio, obras públicas, turismo</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>validation set 1 f1</th>\n",
       "      <td>0.805556</td>\n",
       "      <td>0.951261</td>\n",
       "      <td>0.791667</td>\n",
       "      <td>0.820809</td>\n",
       "      <td>0.815217</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.747664</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.533708</td>\n",
       "      <td>0.885737</td>\n",
       "      <td>0.740260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 2 f1</th>\n",
       "      <td>0.676056</td>\n",
       "      <td>0.937815</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.780488</td>\n",
       "      <td>0.808023</td>\n",
       "      <td>0.725664</td>\n",
       "      <td>0.779661</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.758065</td>\n",
       "      <td>0.787402</td>\n",
       "      <td>0.521212</td>\n",
       "      <td>0.877551</td>\n",
       "      <td>0.716867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 3 f1</th>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.939799</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.814815</td>\n",
       "      <td>0.798883</td>\n",
       "      <td>0.768000</td>\n",
       "      <td>0.675676</td>\n",
       "      <td>0.694737</td>\n",
       "      <td>0.735849</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.543807</td>\n",
       "      <td>0.861044</td>\n",
       "      <td>0.715152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 4 f1</th>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.956229</td>\n",
       "      <td>0.795918</td>\n",
       "      <td>0.814371</td>\n",
       "      <td>0.813953</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.557377</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.722222</td>\n",
       "      <td>0.692913</td>\n",
       "      <td>0.553672</td>\n",
       "      <td>0.872473</td>\n",
       "      <td>0.708861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation sets mean f1</th>\n",
       "      <td>0.750033</td>\n",
       "      <td>0.946276</td>\n",
       "      <td>0.809426</td>\n",
       "      <td>0.807621</td>\n",
       "      <td>0.809019</td>\n",
       "      <td>0.728369</td>\n",
       "      <td>0.677091</td>\n",
       "      <td>0.754593</td>\n",
       "      <td>0.740950</td>\n",
       "      <td>0.711745</td>\n",
       "      <td>0.538100</td>\n",
       "      <td>0.874201</td>\n",
       "      <td>0.720285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation std f1</th>\n",
       "      <td>0.048506</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.036938</td>\n",
       "      <td>0.015870</td>\n",
       "      <td>0.006452</td>\n",
       "      <td>0.027748</td>\n",
       "      <td>0.079370</td>\n",
       "      <td>0.037933</td>\n",
       "      <td>0.013367</td>\n",
       "      <td>0.045411</td>\n",
       "      <td>0.012037</td>\n",
       "      <td>0.008950</td>\n",
       "      <td>0.011912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 1 precision</th>\n",
       "      <td>0.906250</td>\n",
       "      <td>0.969178</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.865854</td>\n",
       "      <td>0.837989</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.705882</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.691176</td>\n",
       "      <td>0.549133</td>\n",
       "      <td>0.903537</td>\n",
       "      <td>0.814286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 2 precision</th>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.962069</td>\n",
       "      <td>0.897436</td>\n",
       "      <td>0.864865</td>\n",
       "      <td>0.849398</td>\n",
       "      <td>0.759259</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.863636</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.585034</td>\n",
       "      <td>0.894400</td>\n",
       "      <td>0.748428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 3 precision</th>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.952542</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.841176</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.657895</td>\n",
       "      <td>0.767442</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.710145</td>\n",
       "      <td>0.604027</td>\n",
       "      <td>0.899329</td>\n",
       "      <td>0.771242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 4 precision</th>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.972603</td>\n",
       "      <td>0.812500</td>\n",
       "      <td>0.871795</td>\n",
       "      <td>0.903226</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.566667</td>\n",
       "      <td>0.724138</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.579882</td>\n",
       "      <td>0.886256</td>\n",
       "      <td>0.761905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation sets mean precision</th>\n",
       "      <td>0.836052</td>\n",
       "      <td>0.964098</td>\n",
       "      <td>0.847697</td>\n",
       "      <td>0.879795</td>\n",
       "      <td>0.857947</td>\n",
       "      <td>0.735244</td>\n",
       "      <td>0.687968</td>\n",
       "      <td>0.813804</td>\n",
       "      <td>0.813147</td>\n",
       "      <td>0.769629</td>\n",
       "      <td>0.579519</td>\n",
       "      <td>0.895880</td>\n",
       "      <td>0.773965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation std precision</th>\n",
       "      <td>0.046237</td>\n",
       "      <td>0.007678</td>\n",
       "      <td>0.038261</td>\n",
       "      <td>0.021452</td>\n",
       "      <td>0.026471</td>\n",
       "      <td>0.029119</td>\n",
       "      <td>0.091857</td>\n",
       "      <td>0.070892</td>\n",
       "      <td>0.046238</td>\n",
       "      <td>0.074474</td>\n",
       "      <td>0.019714</td>\n",
       "      <td>0.006429</td>\n",
       "      <td>0.024651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 1 recall</th>\n",
       "      <td>0.725000</td>\n",
       "      <td>0.933993</td>\n",
       "      <td>0.775510</td>\n",
       "      <td>0.780220</td>\n",
       "      <td>0.793651</td>\n",
       "      <td>0.689655</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.720000</td>\n",
       "      <td>0.655738</td>\n",
       "      <td>0.643836</td>\n",
       "      <td>0.519126</td>\n",
       "      <td>0.868624</td>\n",
       "      <td>0.678571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 2 recall</th>\n",
       "      <td>0.585366</td>\n",
       "      <td>0.914754</td>\n",
       "      <td>0.686275</td>\n",
       "      <td>0.711111</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.694915</td>\n",
       "      <td>0.741935</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.746032</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.469945</td>\n",
       "      <td>0.861325</td>\n",
       "      <td>0.687861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 3 recall</th>\n",
       "      <td>0.717949</td>\n",
       "      <td>0.927393</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.733333</td>\n",
       "      <td>0.760638</td>\n",
       "      <td>0.774194</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.634615</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.690141</td>\n",
       "      <td>0.494505</td>\n",
       "      <td>0.825886</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation set 4 recall</th>\n",
       "      <td>0.697674</td>\n",
       "      <td>0.940397</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>0.764045</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.730159</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>0.807692</td>\n",
       "      <td>0.684211</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.529730</td>\n",
       "      <td>0.859112</td>\n",
       "      <td>0.662722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation sets mean recall</th>\n",
       "      <td>0.681497</td>\n",
       "      <td>0.929134</td>\n",
       "      <td>0.778531</td>\n",
       "      <td>0.747177</td>\n",
       "      <td>0.766380</td>\n",
       "      <td>0.722231</td>\n",
       "      <td>0.667620</td>\n",
       "      <td>0.710220</td>\n",
       "      <td>0.683995</td>\n",
       "      <td>0.664843</td>\n",
       "      <td>0.503327</td>\n",
       "      <td>0.853737</td>\n",
       "      <td>0.673955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation std recall</th>\n",
       "      <td>0.056401</td>\n",
       "      <td>0.009491</td>\n",
       "      <td>0.065808</td>\n",
       "      <td>0.026780</td>\n",
       "      <td>0.019046</td>\n",
       "      <td>0.033802</td>\n",
       "      <td>0.072087</td>\n",
       "      <td>0.063864</td>\n",
       "      <td>0.038088</td>\n",
       "      <td>0.040041</td>\n",
       "      <td>0.023124</td>\n",
       "      <td>0.016460</td>\n",
       "      <td>0.009925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Saúde  Relações Exteriores  Meio ambiente  \\\n",
       "validation set 1 f1             0.805556             0.951261       0.791667   \n",
       "validation set 2 f1             0.676056             0.937815       0.777778   \n",
       "validation set 3 f1             0.777778             0.939799       0.872340   \n",
       "validation set 4 f1             0.740741             0.956229       0.795918   \n",
       "validation sets mean f1         0.750033             0.946276       0.809426   \n",
       "validation std f1               0.048506             0.007705       0.036938   \n",
       "validation set 1 precision      0.906250             0.969178       0.808511   \n",
       "validation set 2 precision      0.800000             0.962069       0.897436   \n",
       "validation set 3 precision      0.848485             0.952542       0.872340   \n",
       "validation set 4 precision      0.789474             0.972603       0.812500   \n",
       "validation sets mean precision  0.836052             0.964098       0.847697   \n",
       "validation std precision        0.046237             0.007678       0.038261   \n",
       "validation set 1 recall         0.725000             0.933993       0.775510   \n",
       "validation set 2 recall         0.585366             0.914754       0.686275   \n",
       "validation set 3 recall         0.717949             0.927393       0.872340   \n",
       "validation set 4 recall         0.697674             0.940397       0.780000   \n",
       "validation sets mean recall     0.681497             0.929134       0.778531   \n",
       "validation std recall           0.056401             0.009491       0.065808   \n",
       "\n",
       "                                Educação, cultura e esporte  \\\n",
       "validation set 1 f1                                0.820809   \n",
       "validation set 2 f1                                0.780488   \n",
       "validation set 3 f1                                0.814815   \n",
       "validation set 4 f1                                0.814371   \n",
       "validation sets mean f1                            0.807621   \n",
       "validation std f1                                  0.015870   \n",
       "validation set 1 precision                         0.865854   \n",
       "validation set 2 precision                         0.864865   \n",
       "validation set 3 precision                         0.916667   \n",
       "validation set 4 precision                         0.871795   \n",
       "validation sets mean precision                     0.879795   \n",
       "validation std precision                           0.021452   \n",
       "validation set 1 recall                            0.780220   \n",
       "validation set 2 recall                            0.711111   \n",
       "validation set 3 recall                            0.733333   \n",
       "validation set 4 recall                            0.764045   \n",
       "validation sets mean recall                        0.747177   \n",
       "validation std recall                              0.026780   \n",
       "\n",
       "                                Justiça e Segurança  Trabalho e Previdência  \\\n",
       "validation set 1 f1                        0.815217                0.689655   \n",
       "validation set 2 f1                        0.808023                0.725664   \n",
       "validation set 3 f1                        0.798883                0.768000   \n",
       "validation set 4 f1                        0.813953                0.730159   \n",
       "validation sets mean f1                    0.809019                0.728369   \n",
       "validation std f1                          0.006452                0.027748   \n",
       "validation set 1 precision                 0.837989                0.689655   \n",
       "validation set 2 precision                 0.849398                0.759259   \n",
       "validation set 3 precision                 0.841176                0.761905   \n",
       "validation set 4 precision                 0.903226                0.730159   \n",
       "validation sets mean precision             0.857947                0.735244   \n",
       "validation std precision                   0.026471                0.029119   \n",
       "validation set 1 recall                    0.793651                0.689655   \n",
       "validation set 2 recall                    0.770492                0.694915   \n",
       "validation set 3 recall                    0.760638                0.774194   \n",
       "validation set 4 recall                    0.740741                0.730159   \n",
       "validation sets mean recall                0.766380                0.722231   \n",
       "validation std recall                      0.019046                0.033802   \n",
       "\n",
       "                                Transporte/transporte de mercadorias  \\\n",
       "validation set 1 f1                                         0.695652   \n",
       "validation set 2 f1                                         0.779661   \n",
       "validation set 3 f1                                         0.675676   \n",
       "validation set 4 f1                                         0.557377   \n",
       "validation sets mean f1                                     0.677091   \n",
       "validation std f1                                           0.079370   \n",
       "validation set 1 precision                                  0.705882   \n",
       "validation set 2 precision                                  0.821429   \n",
       "validation set 3 precision                                  0.657895   \n",
       "validation set 4 precision                                  0.566667   \n",
       "validation sets mean precision                              0.687968   \n",
       "validation std precision                                    0.091857   \n",
       "validation set 1 recall                                     0.685714   \n",
       "validation set 2 recall                                     0.741935   \n",
       "validation set 3 recall                                     0.694444   \n",
       "validation set 4 recall                                     0.548387   \n",
       "validation sets mean recall                                 0.667620   \n",
       "validation std recall                                       0.072087   \n",
       "\n",
       "                                Agricultura, pecuária e pesca  \\\n",
       "validation set 1 f1                                  0.800000   \n",
       "validation set 2 f1                                  0.760000   \n",
       "validation set 3 f1                                  0.694737   \n",
       "validation set 4 f1                                  0.763636   \n",
       "validation sets mean f1                              0.754593   \n",
       "validation std f1                                    0.037933   \n",
       "validation set 1 precision                           0.900000   \n",
       "validation set 2 precision                           0.863636   \n",
       "validation set 3 precision                           0.767442   \n",
       "validation set 4 precision                           0.724138   \n",
       "validation sets mean precision                       0.813804   \n",
       "validation std precision                             0.070892   \n",
       "validation set 1 recall                              0.720000   \n",
       "validation set 2 recall                              0.678571   \n",
       "validation set 3 recall                              0.634615   \n",
       "validation set 4 recall                              0.807692   \n",
       "validation sets mean recall                          0.710220   \n",
       "validation std recall                                0.063864   \n",
       "\n",
       "                                Ciência e tecnologia    Social  Presidência  \\\n",
       "validation set 1 f1                         0.747664  0.666667     0.533708   \n",
       "validation set 2 f1                         0.758065  0.787402     0.521212   \n",
       "validation set 3 f1                         0.735849  0.700000     0.543807   \n",
       "validation set 4 f1                         0.722222  0.692913     0.553672   \n",
       "validation sets mean f1                     0.740950  0.711745     0.538100   \n",
       "validation std f1                           0.013367  0.045411     0.012037   \n",
       "validation set 1 precision                  0.869565  0.691176     0.549133   \n",
       "validation set 2 precision                  0.770492  0.877193     0.585034   \n",
       "validation set 3 precision                  0.847826  0.710145     0.604027   \n",
       "validation set 4 precision                  0.764706  0.800000     0.579882   \n",
       "validation sets mean precision              0.813147  0.769629     0.579519   \n",
       "validation std precision                    0.046238  0.074474     0.019714   \n",
       "validation set 1 recall                     0.655738  0.643836     0.519126   \n",
       "validation set 2 recall                     0.746032  0.714286     0.469945   \n",
       "validation set 3 recall                     0.650000  0.690141     0.494505   \n",
       "validation set 4 recall                     0.684211  0.611111     0.529730   \n",
       "validation sets mean recall                 0.683995  0.664843     0.503327   \n",
       "validation std recall                       0.038088  0.040041     0.023124   \n",
       "\n",
       "                                Economia e planejamento  \\\n",
       "validation set 1 f1                            0.885737   \n",
       "validation set 2 f1                            0.877551   \n",
       "validation set 3 f1                            0.861044   \n",
       "validation set 4 f1                            0.872473   \n",
       "validation sets mean f1                        0.874201   \n",
       "validation std f1                              0.008950   \n",
       "validation set 1 precision                     0.903537   \n",
       "validation set 2 precision                     0.894400   \n",
       "validation set 3 precision                     0.899329   \n",
       "validation set 4 precision                     0.886256   \n",
       "validation sets mean precision                 0.895880   \n",
       "validation std precision                       0.006429   \n",
       "validation set 1 recall                        0.868624   \n",
       "validation set 2 recall                        0.861325   \n",
       "validation set 3 recall                        0.825886   \n",
       "validation set 4 recall                        0.859112   \n",
       "validation sets mean recall                    0.853737   \n",
       "validation std recall                          0.016460   \n",
       "\n",
       "                                Indústria, comércio, obras públicas, turismo  \n",
       "validation set 1 f1                                                 0.740260  \n",
       "validation set 2 f1                                                 0.716867  \n",
       "validation set 3 f1                                                 0.715152  \n",
       "validation set 4 f1                                                 0.708861  \n",
       "validation sets mean f1                                             0.720285  \n",
       "validation std f1                                                   0.011912  \n",
       "validation set 1 precision                                          0.814286  \n",
       "validation set 2 precision                                          0.748428  \n",
       "validation set 3 precision                                          0.771242  \n",
       "validation set 4 precision                                          0.761905  \n",
       "validation sets mean precision                                      0.773965  \n",
       "validation std precision                                            0.024651  \n",
       "validation set 1 recall                                             0.678571  \n",
       "validation set 2 recall                                             0.687861  \n",
       "validation set 3 recall                                             0.666667  \n",
       "validation set 4 recall                                             0.662722  \n",
       "validation sets mean recall                                         0.673955  \n",
       "validation std recall                                               0.009925  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_results_minist = pd.read_csv(\"optimization/validation_results/HAN_minist_v2iteration_\"+str(best_parameters_idx_minist)+\".csv\", index_col=0)\n",
    "validation_results_minist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final \"referenda\" evaluation on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/11\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 1.7957 - get_f1: 0.1838\n",
      "Epoch 2/11\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 1.0879 - get_f1: 0.6656\n",
      "Epoch 3/11\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 0.6733 - get_f1: 0.8015\n",
      "Epoch 4/11\n",
      "87/87 [==============================] - 8s 96ms/step - loss: 0.4453 - get_f1: 0.8621\n",
      "Epoch 5/11\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 0.3117 - get_f1: 0.9007\n",
      "Epoch 6/11\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 0.2063 - get_f1: 0.9337\n",
      "Epoch 7/11\n",
      "87/87 [==============================] - 8s 94ms/step - loss: 0.1340 - get_f1: 0.9568\n",
      "Epoch 8/11\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 0.0891 - get_f1: 0.9715\n",
      "Epoch 9/11\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 0.0521 - get_f1: 0.9832\n",
      "Epoch 10/11\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 0.0309 - get_f1: 0.9908\n",
      "Epoch 11/11\n",
      "87/87 [==============================] - 8s 95ms/step - loss: 0.0201 - get_f1: 0.9943\n",
      "Referenda - HAN tuned\n",
      "                                              precision    recall  f1-score   support\n",
      "\n",
      "                                       Saúde      0.659     0.643     0.651        42\n",
      "                         Relações Exteriores      0.963     0.935     0.949       308\n",
      "                               Meio ambiente      0.889     0.696     0.780        46\n",
      "                 Educação, cultura e esporte      0.889     0.719     0.795        89\n",
      "                         Justiça e Segurança      0.801     0.754     0.777       187\n",
      "                      Trabalho e Previdência      0.719     0.695     0.707        59\n",
      "        Transporte/transporte de mercadorias      0.722     0.867     0.788        30\n",
      "               Agricultura, pecuária e pesca      0.846     0.611     0.710        54\n",
      "                        Ciência e tecnologia      0.804     0.683     0.739        60\n",
      "                                      Social      0.781     0.704     0.741        71\n",
      "                                 Presidência      0.526     0.492     0.508       183\n",
      "                     Economia e planejamento      0.920     0.867     0.893       653\n",
      "Indústria, comércio, obras públicas, turismo      0.795     0.717     0.754       173\n",
      "\n",
      "                                   micro avg      0.840     0.779     0.808      1955\n",
      "                                   macro avg      0.793     0.722     0.753      1955\n",
      "                                weighted avg      0.840     0.779     0.807      1955\n",
      "                                 samples avg      0.852     0.831     0.818      1955\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/caiocampos/.local/lib/python3.6/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "beta_2_real_p = [0.99,0.999,0.9999]\n",
    "\n",
    "Train_Y_minist = np.array([np.array(t) for t in Train_Y_minist])\n",
    "Test_Y_minist = np.array([np.array(t) for t in Test_Y_minist])\n",
    "\n",
    "model = HAN_model(len(Train_Y_minist[0]))\n",
    "\n",
    "opt = Adam(lr = parameters_minist[\"learning_rate\"].iloc[0], beta_1 = parameters_minist[\"beta1\"].iloc[0], beta_2 = beta_2_real_p[int(parameters_minist[\"beta2\"].iloc[0])])\n",
    "model.compile(loss='binary_crossentropy', optimizer=opt, metrics=[get_f1])\n",
    "history = model.fit(x=np.array(Train_X), y=np.array(Train_Y_minist), epochs=int(parameters_minist[\"epochs\"].iloc[0]), validation_split=0.0, class_weight=classes_w_minist, batch_size=int(parameters_minist[\"batch_size\"].iloc[0]), shuffle=False)\n",
    "test_prediction = model.predict(np.array(Test_X))\n",
    "print(\"Referenda - HAN tuned\")\n",
    "print(classification_report(y_true=np.array(Test_Y_minist), y_pred=predict_classes(test_prediction), digits=3, target_names=minist_classes_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: models/HAN_minist_model_final/assets\n"
     ]
    }
   ],
   "source": [
    "#Saving final model\n",
    "model.save('models/HAN_minist_model_final')\n",
    "pd.DataFrame(predict_classes(test_prediction)).to_csv(\"test_results/HAN_minist.csv\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/13 classes: 0.0\n",
      "1/13 classes: 0.0\n",
      "2/13 classes: 0.0\n",
      "3/13 classes: 0.0\n",
      "4/13 classes: 0.0\n",
      "5/13 classes: 0.07457121551081282\n",
      "6/13 classes: 0.07457121551081282\n",
      "7/13 classes: 0.0\n",
      "8/13 classes: 0.2982848620432513\n",
      "9/13 classes: 0.5219985085756897\n",
      "10/13 classes: 2.460850111856823\n",
      "11/13 classes: 10.216256524981358\n",
      "12/13 classes: 21.327367636092468\n",
      "13/13 classes: 65.02609992542878\n"
     ]
    }
   ],
   "source": [
    "for i in range (len(Test_Y_minist[0])+1):\n",
    "    print(str(i)+\"/\"+str(len(Test_Y_minist[0]))+\" classes:\", acerto_absoluto(Test_Y_minist, test_prediction, i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n",
      "526\n",
      "527\n",
      "528\n",
      "529\n",
      "530\n",
      "531\n",
      "532\n",
      "533\n",
      "534\n",
      "535\n",
      "536\n",
      "537\n",
      "538\n",
      "539\n",
      "540\n",
      "541\n",
      "542\n",
      "543\n",
      "544\n",
      "545\n",
      "546\n",
      "547\n",
      "548\n",
      "549\n",
      "550\n",
      "551\n",
      "552\n",
      "553\n",
      "554\n",
      "555\n",
      "556\n",
      "557\n",
      "558\n",
      "559\n",
      "560\n",
      "561\n",
      "562\n",
      "563\n",
      "564\n",
      "565\n",
      "566\n",
      "567\n",
      "568\n",
      "569\n",
      "570\n",
      "571\n",
      "572\n",
      "573\n",
      "574\n",
      "575\n",
      "576\n",
      "577\n",
      "578\n",
      "579\n",
      "580\n",
      "581\n",
      "582\n",
      "583\n",
      "584\n",
      "585\n",
      "586\n",
      "587\n",
      "588\n",
      "589\n",
      "590\n",
      "591\n",
      "592\n",
      "593\n",
      "594\n",
      "595\n",
      "596\n",
      "597\n",
      "598\n",
      "599\n",
      "600\n",
      "601\n",
      "602\n",
      "603\n",
      "604\n",
      "605\n",
      "606\n",
      "607\n",
      "608\n",
      "609\n",
      "610\n",
      "611\n",
      "612\n",
      "613\n",
      "614\n",
      "615\n",
      "616\n",
      "617\n",
      "618\n",
      "619\n",
      "620\n",
      "621\n",
      "622\n",
      "623\n",
      "624\n",
      "625\n",
      "626\n",
      "627\n",
      "628\n",
      "629\n",
      "630\n",
      "631\n",
      "632\n",
      "633\n",
      "634\n",
      "635\n",
      "636\n",
      "637\n",
      "638\n",
      "639\n",
      "640\n",
      "641\n",
      "642\n",
      "643\n",
      "644\n",
      "645\n",
      "646\n",
      "647\n",
      "648\n",
      "649\n",
      "650\n",
      "651\n",
      "652\n",
      "653\n",
      "654\n",
      "655\n",
      "656\n",
      "657\n",
      "658\n",
      "659\n",
      "660\n",
      "661\n",
      "662\n",
      "663\n",
      "664\n",
      "665\n",
      "666\n",
      "667\n",
      "668\n",
      "669\n",
      "670\n",
      "671\n",
      "672\n",
      "673\n",
      "674\n",
      "675\n",
      "676\n",
      "677\n",
      "678\n",
      "679\n",
      "680\n",
      "681\n",
      "682\n",
      "683\n",
      "684\n",
      "685\n",
      "686\n",
      "687\n",
      "688\n",
      "689\n",
      "690\n",
      "691\n",
      "692\n",
      "693\n",
      "694\n",
      "695\n",
      "696\n",
      "697\n",
      "698\n",
      "699\n",
      "700\n",
      "701\n",
      "702\n",
      "703\n",
      "704\n",
      "705\n",
      "706\n",
      "707\n",
      "708\n",
      "709\n",
      "710\n",
      "711\n",
      "712\n",
      "713\n",
      "714\n",
      "715\n",
      "716\n",
      "717\n",
      "718\n",
      "719\n",
      "720\n",
      "721\n",
      "722\n",
      "723\n",
      "724\n",
      "725\n",
      "726\n",
      "727\n",
      "728\n",
      "729\n",
      "730\n",
      "731\n",
      "732\n",
      "733\n",
      "734\n",
      "735\n",
      "736\n",
      "737\n",
      "738\n",
      "739\n",
      "740\n",
      "741\n",
      "742\n",
      "743\n",
      "744\n",
      "745\n",
      "746\n",
      "747\n",
      "748\n",
      "749\n",
      "750\n",
      "751\n",
      "752\n",
      "753\n",
      "754\n",
      "755\n",
      "756\n",
      "757\n",
      "758\n",
      "759\n",
      "760\n",
      "761\n",
      "762\n",
      "763\n",
      "764\n",
      "765\n",
      "766\n",
      "767\n",
      "768\n",
      "769\n",
      "770\n",
      "771\n",
      "772\n",
      "773\n",
      "774\n",
      "775\n",
      "776\n",
      "777\n",
      "778\n",
      "779\n",
      "780\n",
      "781\n",
      "782\n",
      "783\n",
      "784\n",
      "785\n",
      "786\n",
      "787\n",
      "788\n",
      "789\n",
      "790\n",
      "791\n",
      "792\n",
      "793\n",
      "794\n",
      "795\n",
      "796\n",
      "797\n",
      "798\n",
      "799\n",
      "800\n",
      "801\n",
      "802\n",
      "803\n",
      "804\n",
      "805\n",
      "806\n",
      "807\n",
      "808\n",
      "809\n",
      "810\n",
      "811\n",
      "812\n",
      "813\n",
      "814\n",
      "815\n",
      "816\n",
      "817\n",
      "818\n",
      "819\n",
      "820\n",
      "821\n",
      "822\n",
      "823\n",
      "824\n",
      "825\n",
      "826\n",
      "827\n",
      "828\n",
      "829\n",
      "830\n",
      "831\n",
      "832\n",
      "833\n",
      "834\n",
      "835\n",
      "836\n",
      "837\n",
      "838\n",
      "839\n",
      "840\n",
      "841\n",
      "842\n",
      "843\n",
      "844\n",
      "845\n",
      "846\n",
      "847\n",
      "848\n",
      "849\n",
      "850\n",
      "851\n",
      "852\n",
      "853\n",
      "854\n",
      "855\n",
      "856\n",
      "857\n",
      "858\n",
      "859\n",
      "860\n",
      "861\n",
      "862\n",
      "863\n",
      "864\n",
      "865\n",
      "866\n",
      "867\n",
      "868\n",
      "869\n",
      "870\n",
      "871\n",
      "872\n",
      "873\n",
      "874\n",
      "875\n",
      "876\n",
      "877\n",
      "878\n",
      "879\n",
      "880\n",
      "881\n",
      "882\n",
      "883\n",
      "884\n",
      "885\n",
      "886\n",
      "887\n",
      "888\n",
      "889\n",
      "890\n",
      "891\n",
      "892\n",
      "893\n",
      "894\n",
      "895\n",
      "896\n",
      "897\n",
      "898\n",
      "899\n",
      "900\n",
      "901\n",
      "902\n",
      "903\n",
      "904\n",
      "905\n",
      "906\n",
      "907\n",
      "908\n",
      "909\n",
      "910\n",
      "911\n",
      "912\n",
      "913\n",
      "914\n",
      "915\n",
      "916\n",
      "917\n",
      "918\n",
      "919\n",
      "920\n",
      "921\n",
      "922\n",
      "923\n",
      "924\n",
      "925\n",
      "926\n",
      "927\n",
      "928\n",
      "929\n",
      "930\n",
      "931\n",
      "932\n",
      "933\n",
      "934\n",
      "935\n",
      "936\n",
      "937\n",
      "938\n",
      "939\n",
      "940\n",
      "941\n",
      "942\n",
      "943\n",
      "944\n",
      "945\n",
      "946\n",
      "947\n",
      "948\n",
      "949\n",
      "950\n",
      "951\n",
      "952\n",
      "953\n",
      "954\n",
      "955\n",
      "956\n",
      "957\n",
      "958\n",
      "959\n",
      "960\n",
      "961\n",
      "962\n",
      "963\n",
      "964\n",
      "965\n",
      "966\n",
      "967\n",
      "968\n",
      "969\n",
      "970\n",
      "971\n",
      "972\n",
      "973\n",
      "974\n",
      "975\n",
      "976\n",
      "977\n",
      "978\n",
      "979\n",
      "980\n",
      "981\n",
      "982\n",
      "983\n",
      "984\n",
      "985\n",
      "986\n",
      "987\n",
      "988\n",
      "989\n",
      "990\n",
      "991\n",
      "992\n",
      "993\n",
      "994\n",
      "995\n",
      "996\n",
      "997\n",
      "998\n",
      "999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7535569949238192,\n",
       " 0.755835500188393,\n",
       " 0.7481494309729033,\n",
       " 0.749235024049083,\n",
       " 0.7538622090395753,\n",
       " 0.7590434066814411,\n",
       " 0.7558045958747882,\n",
       " 0.7585466624916026,\n",
       " 0.7489860697348474,\n",
       " 0.7592917164911888,\n",
       " 0.7438097342916463,\n",
       " 0.7524325720285726,\n",
       " 0.7385083521044803,\n",
       " 0.7335889848301598,\n",
       " 0.7772116088387035,\n",
       " 0.7646975215748246,\n",
       " 0.7321008029375116,\n",
       " 0.7215871434824174,\n",
       " 0.7444574700062662,\n",
       " 0.7404383389096297,\n",
       " 0.7600252793224386,\n",
       " 0.730226655818043,\n",
       " 0.7659975631917183,\n",
       " 0.7569556921978386,\n",
       " 0.7351977827054008,\n",
       " 0.7341392159050214,\n",
       " 0.748006490978531,\n",
       " 0.7442362013895227,\n",
       " 0.7592114717522389,\n",
       " 0.7226346815412149,\n",
       " 0.7446999852111242,\n",
       " 0.7573149034222215,\n",
       " 0.7415480722129904,\n",
       " 0.7406342451536787,\n",
       " 0.7674022154970284,\n",
       " 0.7440724547784957,\n",
       " 0.7416842587812505,\n",
       " 0.7634625398738092,\n",
       " 0.7496193643267136,\n",
       " 0.7324258077848971,\n",
       " 0.7599136708905003,\n",
       " 0.7469368613669727,\n",
       " 0.7432949181583728,\n",
       " 0.7707679728803049,\n",
       " 0.7299120652524264,\n",
       " 0.7575447672713901,\n",
       " 0.7206247676611154,\n",
       " 0.7516745496228622,\n",
       " 0.7683052533557574,\n",
       " 0.7436646632837541,\n",
       " 0.7523334035162001,\n",
       " 0.7541891216750449,\n",
       " 0.7544627451329767,\n",
       " 0.7491585671971084,\n",
       " 0.7419668544939574,\n",
       " 0.7514939410580018,\n",
       " 0.7487031216227541,\n",
       " 0.7403998889650379,\n",
       " 0.7449313760772861,\n",
       " 0.7484715592197746,\n",
       " 0.7467461046821386,\n",
       " 0.7349802583670407,\n",
       " 0.752077042994334,\n",
       " 0.7362444915627826,\n",
       " 0.7655294208343258,\n",
       " 0.7671551062177999,\n",
       " 0.7267991866095452,\n",
       " 0.7515613566137407,\n",
       " 0.7485584071850897,\n",
       " 0.7822488249123428,\n",
       " 0.7395602251647551,\n",
       " 0.759752119944135,\n",
       " 0.7375106194409912,\n",
       " 0.7393145150803685,\n",
       " 0.7850311169013726,\n",
       " 0.7505581210654083,\n",
       " 0.7561195612685997,\n",
       " 0.7438003146921304,\n",
       " 0.7423212429509657,\n",
       " 0.7507287618725915,\n",
       " 0.7680364650810552,\n",
       " 0.7525570252728762,\n",
       " 0.7380265204320782,\n",
       " 0.7595082430116544,\n",
       " 0.7571195379184636,\n",
       " 0.7648056502170884,\n",
       " 0.7474251942560036,\n",
       " 0.7524192296313985,\n",
       " 0.7527143844624612,\n",
       " 0.7555174623088381,\n",
       " 0.763301351767172,\n",
       " 0.756807932950736,\n",
       " 0.7576460022212695,\n",
       " 0.7702377089424769,\n",
       " 0.7587756773162011,\n",
       " 0.7487066761337614,\n",
       " 0.7570948620124826,\n",
       " 0.7384204677896233,\n",
       " 0.7520417036577464,\n",
       " 0.7499301321588052,\n",
       " 0.7456939826715996,\n",
       " 0.7377342723801015,\n",
       " 0.7587188751392705,\n",
       " 0.7497646378444617,\n",
       " 0.7473803282787632,\n",
       " 0.7398001694703861,\n",
       " 0.7227208783889393,\n",
       " 0.7386548937712238,\n",
       " 0.74794079195704,\n",
       " 0.7523408600517357,\n",
       " 0.7565724678451193,\n",
       " 0.7614189167949694,\n",
       " 0.7447643030393023,\n",
       " 0.7496891637320251,\n",
       " 0.775297182985449,\n",
       " 0.7636917399994214,\n",
       " 0.7575098748187572,\n",
       " 0.7643944387946575,\n",
       " 0.740115705198658,\n",
       " 0.7673018586687964,\n",
       " 0.7480251991262564,\n",
       " 0.7418810605208186,\n",
       " 0.7440773959104896,\n",
       " 0.7631497106961785,\n",
       " 0.779398742869182,\n",
       " 0.7459855855328693,\n",
       " 0.7577595753426181,\n",
       " 0.7512366692077843,\n",
       " 0.7609524981835922,\n",
       " 0.7537461068211164,\n",
       " 0.7588923328416003,\n",
       " 0.7562341059487304,\n",
       " 0.7495570289155906,\n",
       " 0.7496263349296647,\n",
       " 0.754743245741727,\n",
       " 0.7403931118982086,\n",
       " 0.7540320710571978,\n",
       " 0.757008225362055,\n",
       " 0.7640735624517995,\n",
       " 0.7423051270028783,\n",
       " 0.7449051019924355,\n",
       " 0.7426250018006265,\n",
       " 0.7776559648465592,\n",
       " 0.7521931411119068,\n",
       " 0.7481800470127554,\n",
       " 0.7484031194320772,\n",
       " 0.781908201758619,\n",
       " 0.7653524289933435,\n",
       " 0.7574852795000155,\n",
       " 0.722135753172136,\n",
       " 0.7429173948454336,\n",
       " 0.748242956913913,\n",
       " 0.7554859185742449,\n",
       " 0.7591017991135263,\n",
       " 0.7419573774223746,\n",
       " 0.7471363439045899,\n",
       " 0.7433355595121599,\n",
       " 0.7326048324094494,\n",
       " 0.7451281912666308,\n",
       " 0.7556373020744557,\n",
       " 0.7403265480932318,\n",
       " 0.7314558349363439,\n",
       " 0.7620560569365823,\n",
       " 0.7578797159713433,\n",
       " 0.7627222784191415,\n",
       " 0.7361550434027865,\n",
       " 0.7614907257391808,\n",
       " 0.74631693900443,\n",
       " 0.7536686093407556,\n",
       " 0.7452782555674238,\n",
       " 0.7646706786223277,\n",
       " 0.7250611351794679,\n",
       " 0.7716727624053046,\n",
       " 0.7707777741612527,\n",
       " 0.7499179936009815,\n",
       " 0.7544730210412232,\n",
       " 0.7676543568730616,\n",
       " 0.7556181438185076,\n",
       " 0.7509945825050527,\n",
       " 0.7585539248993445,\n",
       " 0.7537878095921323,\n",
       " 0.7435354150341524,\n",
       " 0.7458956853599406,\n",
       " 0.7336119395996743,\n",
       " 0.755470520481266,\n",
       " 0.7495593099471962,\n",
       " 0.7742922838660639,\n",
       " 0.7274833779388477,\n",
       " 0.7541469844720935,\n",
       " 0.75824645948609,\n",
       " 0.7527908210575627,\n",
       " 0.7677913420337263,\n",
       " 0.7407458899523565,\n",
       " 0.7583835402948638,\n",
       " 0.7479849086263658,\n",
       " 0.7360229834280125,\n",
       " 0.7455729959792069,\n",
       " 0.7716333300204197,\n",
       " 0.7460037895210996,\n",
       " 0.7579042252709569,\n",
       " 0.7683909878656693,\n",
       " 0.7437721471716778,\n",
       " 0.7693792226947231,\n",
       " 0.7757885760566918,\n",
       " 0.7480330738973934,\n",
       " 0.7660364948534846,\n",
       " 0.7402308020196064,\n",
       " 0.7441334531315081,\n",
       " 0.7445429172855828,\n",
       " 0.7565239538719021,\n",
       " 0.7595908913873759,\n",
       " 0.76839974952302,\n",
       " 0.75767474182258,\n",
       " 0.7328926288525499,\n",
       " 0.7638884697213507,\n",
       " 0.7728753237438687,\n",
       " 0.7497770851828439,\n",
       " 0.7762818747042842,\n",
       " 0.7497429957802915,\n",
       " 0.7508947635176045,\n",
       " 0.7614353994485832,\n",
       " 0.7607409328669851,\n",
       " 0.7686949802235226,\n",
       " 0.7584401680513468,\n",
       " 0.7474642121050807,\n",
       " 0.745128673154124,\n",
       " 0.7537838359619189,\n",
       " 0.7631327985208701,\n",
       " 0.7626238253753416,\n",
       " 0.7502608666216558,\n",
       " 0.7350327489797017,\n",
       " 0.7538967493357913,\n",
       " 0.745876134294253,\n",
       " 0.7092470382061974,\n",
       " 0.7730442383427374,\n",
       " 0.7416612848906384,\n",
       " 0.7451751945431005,\n",
       " 0.745099415246911,\n",
       " 0.7466176311602687,\n",
       " 0.741321380255667,\n",
       " 0.7537705853754647,\n",
       " 0.7528681382839473,\n",
       " 0.7597093221710514,\n",
       " 0.7655370995865843,\n",
       " 0.7217391954327659,\n",
       " 0.7499003056055299,\n",
       " 0.7373288493470048,\n",
       " 0.757100801595548,\n",
       " 0.751112777949918,\n",
       " 0.7372805617969931,\n",
       " 0.7796562866432247,\n",
       " 0.763853266811993,\n",
       " 0.7654410921380628,\n",
       " 0.7477013696383981,\n",
       " 0.7481739120699074,\n",
       " 0.7389016881503335,\n",
       " 0.7506496161026903,\n",
       " 0.7447971907710068,\n",
       " 0.7474517761774482,\n",
       " 0.775786035595327,\n",
       " 0.7641621770917594,\n",
       " 0.7450008564553381,\n",
       " 0.7520320510407829,\n",
       " 0.7431306964092624,\n",
       " 0.749908811302213,\n",
       " 0.7489708582495702,\n",
       " 0.7465172722105151,\n",
       " 0.753398555850086,\n",
       " 0.7505864347132996,\n",
       " 0.744558374020118,\n",
       " 0.7842156152007695,\n",
       " 0.7464834757590952,\n",
       " 0.7497256036045652,\n",
       " 0.734338584623264,\n",
       " 0.75071412815715,\n",
       " 0.7718239023150146,\n",
       " 0.7611914457938395,\n",
       " 0.7244015986341907,\n",
       " 0.7435249397174197,\n",
       " 0.7656540257750801,\n",
       " 0.7477508999232076,\n",
       " 0.7481702116070253,\n",
       " 0.7470257648123481,\n",
       " 0.7547610165477027,\n",
       " 0.7368416609256534,\n",
       " 0.7682432129796509,\n",
       " 0.7697331641581826,\n",
       " 0.7616135802586389,\n",
       " 0.7666733522942591,\n",
       " 0.7531216564956226,\n",
       " 0.7354237052702574,\n",
       " 0.7601397819191948,\n",
       " 0.7361732908577513,\n",
       " 0.7433563905363704,\n",
       " 0.7451027743664907,\n",
       " 0.7592028489429554,\n",
       " 0.7610747598351286,\n",
       " 0.7332623334410352,\n",
       " 0.7737576800321483,\n",
       " 0.7514418735030339,\n",
       " 0.7680542024780732,\n",
       " 0.7584827648619263,\n",
       " 0.7426320141032566,\n",
       " 0.7504685494572022,\n",
       " 0.7392339559921093,\n",
       " 0.7509510024615887,\n",
       " 0.7451862450425126,\n",
       " 0.7491735654312267,\n",
       " 0.7536811171841973,\n",
       " 0.7365128789281964,\n",
       " 0.747399691997429,\n",
       " 0.7329108600472222,\n",
       " 0.7554492790199935,\n",
       " 0.755845629035615,\n",
       " 0.7364188240856309,\n",
       " 0.7341161211390916,\n",
       " 0.7383112915780359,\n",
       " 0.7540840963310906,\n",
       " 0.7684060784722371,\n",
       " 0.750742637199316,\n",
       " 0.7479065681391148,\n",
       " 0.7379008973513871,\n",
       " 0.7634457763220419,\n",
       " 0.7779460063726972,\n",
       " 0.7563244588670005,\n",
       " 0.7635614206149787,\n",
       " 0.7165518940724843,\n",
       " 0.7186867343584095,\n",
       " 0.7551060440456456,\n",
       " 0.7437312022006376,\n",
       " 0.773471187707551,\n",
       " 0.7332139730872564,\n",
       " 0.7511173585961871,\n",
       " 0.7455958861403527,\n",
       " 0.7632116065723638,\n",
       " 0.750577452703289,\n",
       " 0.7455251194829221,\n",
       " 0.747748208501764,\n",
       " 0.737575497821256,\n",
       " 0.7598227805545574,\n",
       " 0.747025439102589,\n",
       " 0.7466785614206113,\n",
       " 0.7698890515794706,\n",
       " 0.7435600026225418,\n",
       " 0.770637518151704,\n",
       " 0.7663814516343874,\n",
       " 0.748909178193042,\n",
       " 0.7459079253363533,\n",
       " 0.757057407425436,\n",
       " 0.751103850112767,\n",
       " 0.7465439278441935,\n",
       " 0.7653411229284213,\n",
       " 0.7534827191129215,\n",
       " 0.7610543341367736,\n",
       " 0.7450142609155848,\n",
       " 0.745445579339636,\n",
       " 0.7424245299716596,\n",
       " 0.7548488666252329,\n",
       " 0.7539885035131709,\n",
       " 0.7645317804165331,\n",
       " 0.7818427312388933,\n",
       " 0.7588839612601479,\n",
       " 0.7401937199712817,\n",
       " 0.76716164373129,\n",
       " 0.7667653329986422,\n",
       " 0.7694268690871247,\n",
       " 0.7555165081059858,\n",
       " 0.7337816137956418,\n",
       " 0.7535889466665912,\n",
       " 0.7549013951737251,\n",
       " 0.770501018931916,\n",
       " 0.7420324877093958,\n",
       " 0.7497777559701162,\n",
       " 0.7619597797094185,\n",
       " 0.7447718950026314,\n",
       " 0.7511000097436279,\n",
       " 0.7634890108599895,\n",
       " 0.7232785692090622,\n",
       " 0.7600056657740987,\n",
       " 0.7565458457011085,\n",
       " 0.7622396892740269,\n",
       " 0.7718866208104946,\n",
       " 0.7305180221346679,\n",
       " 0.766828922919365,\n",
       " 0.7278793065015241,\n",
       " 0.7395179349848415,\n",
       " 0.7330658448444385,\n",
       " 0.7508947609471843,\n",
       " 0.7497033902945827,\n",
       " 0.7585484107301188,\n",
       " 0.7530360053540653,\n",
       " 0.7591807553699186,\n",
       " 0.7552902153938992,\n",
       " 0.7711935039281506,\n",
       " 0.7484694400703005,\n",
       " 0.7722601960493543,\n",
       " 0.7440042317310677,\n",
       " 0.7244302079378897,\n",
       " 0.7437086594647542,\n",
       " 0.7445571329260066,\n",
       " 0.7739391341717964,\n",
       " 0.7463310988245779,\n",
       " 0.7651012742177877,\n",
       " 0.7621696954027138,\n",
       " 0.7446870588723727,\n",
       " 0.7459240372672562,\n",
       " 0.7542960809018131,\n",
       " 0.7467073979472634,\n",
       " 0.7722931773576625,\n",
       " 0.7433254041246553,\n",
       " 0.7622761047835112,\n",
       " 0.7382377375044629,\n",
       " 0.7554060764684978,\n",
       " 0.7734898619763383,\n",
       " 0.7543391385500079,\n",
       " 0.7303103569702952,\n",
       " 0.7665709262857684,\n",
       " 0.7610091448225258,\n",
       " 0.754857300182787,\n",
       " 0.741143183715561,\n",
       " 0.7567351743329229,\n",
       " 0.7684882021608465,\n",
       " 0.7449654297670597,\n",
       " 0.7410172032271927,\n",
       " 0.7495907919757924,\n",
       " 0.7267197917439785,\n",
       " 0.768722668587714,\n",
       " 0.7626266031913128,\n",
       " 0.7375401404216222,\n",
       " 0.7412894389419535,\n",
       " 0.7409974904301871,\n",
       " 0.7410940453693927,\n",
       " 0.7763808128220963,\n",
       " 0.7475073103693562,\n",
       " 0.7559493649269551,\n",
       " 0.771181555507717,\n",
       " 0.7309875217359102,\n",
       " 0.7626757042996226,\n",
       " 0.7370476032049204,\n",
       " 0.7440945406523851,\n",
       " 0.7493391725896799,\n",
       " 0.7680372869560469,\n",
       " 0.7567646643278167,\n",
       " 0.7334789518894366,\n",
       " 0.7465254440952206,\n",
       " 0.7486975203486594,\n",
       " 0.7597792473115237,\n",
       " 0.7455015272979376,\n",
       " 0.735031916794896,\n",
       " 0.7554768499188268,\n",
       " 0.7605156704293161,\n",
       " 0.7491981079484904,\n",
       " 0.7317273237873434,\n",
       " 0.7452812525876931,\n",
       " 0.7580981206327768,\n",
       " 0.7463946650010957,\n",
       " 0.7469527768091493,\n",
       " 0.7482460171599858,\n",
       " 0.751478138465379,\n",
       " 0.732453817521825,\n",
       " 0.778437219473975,\n",
       " 0.7582285492386119,\n",
       " 0.7258017681195063,\n",
       " 0.7524226504531016,\n",
       " 0.7588124479090432,\n",
       " 0.7398184208314523,\n",
       " 0.7345583859601447,\n",
       " 0.7724261188435304,\n",
       " 0.7593276850491985,\n",
       " 0.7423607777925858,\n",
       " 0.7705117366318177,\n",
       " 0.7698276634339022,\n",
       " 0.7704220316679614,\n",
       " 0.7814545982209502,\n",
       " 0.7419410680711865,\n",
       " 0.7500234521899242,\n",
       " 0.76059008314953,\n",
       " 0.7494506293082878,\n",
       " 0.7443578029498348,\n",
       " 0.73489230531095,\n",
       " 0.7429960921637456,\n",
       " 0.7558287139041391,\n",
       " 0.7338209996109055,\n",
       " 0.739980497684731,\n",
       " 0.743699241084456,\n",
       " 0.7610989865781431,\n",
       " 0.7577567562027847,\n",
       " 0.7479633654446486,\n",
       " 0.753463413912937,\n",
       " 0.7354398455120477,\n",
       " 0.7208515570112481,\n",
       " 0.7733192125989737,\n",
       " 0.7493362253375911,\n",
       " 0.7507992982912897,\n",
       " 0.7392408720079767,\n",
       " 0.7428301610004874,\n",
       " 0.75295880596658,\n",
       " 0.7215518641495448,\n",
       " 0.7460767476818526,\n",
       " 0.7587814334256344,\n",
       " 0.7606914404231009,\n",
       " 0.7761980624491467,\n",
       " 0.7572047581104877,\n",
       " 0.7669923778923113,\n",
       " 0.7525257178535054,\n",
       " 0.7567684384746857,\n",
       " 0.7402769665361999,\n",
       " 0.752412137518222,\n",
       " 0.7729375517793405,\n",
       " 0.721585835907712,\n",
       " 0.7388594657173365,\n",
       " 0.7387232587858175,\n",
       " 0.7645377550285415,\n",
       " 0.7624550272836453,\n",
       " 0.7572116335269473,\n",
       " 0.7241171479771153,\n",
       " 0.7440668928434663,\n",
       " 0.7561887482611325,\n",
       " 0.7621954592170987,\n",
       " 0.7717244002656943,\n",
       " 0.770534539028408,\n",
       " 0.7439920922117362,\n",
       " 0.7732322132043775,\n",
       " 0.7483534760556169,\n",
       " 0.755051283945356,\n",
       " 0.75879040904605,\n",
       " 0.7497266652759399,\n",
       " 0.7717444986773343,\n",
       " 0.7478357498666793,\n",
       " 0.7404440078884622,\n",
       " 0.7391633271075313,\n",
       " 0.7603265307350208,\n",
       " 0.7622698373836337,\n",
       " 0.7548404096049898,\n",
       " 0.7652290769862564,\n",
       " 0.7597262912242282,\n",
       " 0.7447173399300964,\n",
       " 0.7465551819352366,\n",
       " 0.7540011008368362,\n",
       " 0.744180421454106,\n",
       " 0.7546868189027632,\n",
       " 0.7461735305612784,\n",
       " 0.7781988519107765,\n",
       " 0.7446470049282858,\n",
       " 0.7613017430752131,\n",
       " 0.7313431474903809,\n",
       " 0.7600116765278242,\n",
       " 0.7396923559817861,\n",
       " 0.7569156280520175,\n",
       " 0.738272541003031,\n",
       " 0.7492836476691468,\n",
       " 0.7458769789718059,\n",
       " 0.7465420906483448,\n",
       " 0.7671373533050642,\n",
       " 0.7706956377415269,\n",
       " 0.7401915441452893,\n",
       " 0.7374540678745054,\n",
       " 0.7484415435553237,\n",
       " 0.7574146191195956,\n",
       " 0.7395607284910691,\n",
       " 0.7502092761458499,\n",
       " 0.7462643094093624,\n",
       " 0.7502198845848462,\n",
       " 0.7600676850264191,\n",
       " 0.7509408687953525,\n",
       " 0.7501811090300415,\n",
       " 0.7589602877181373,\n",
       " 0.7535727348792436,\n",
       " 0.7733887250697242,\n",
       " 0.7270710227085163,\n",
       " 0.7468867034306298,\n",
       " 0.7736577927947919,\n",
       " 0.7593381203062537,\n",
       " 0.7509712452390155,\n",
       " 0.74396948793871,\n",
       " 0.7453054779114285,\n",
       " 0.7614119007007338,\n",
       " 0.7610704100722543,\n",
       " 0.7757543210884232,\n",
       " 0.7650946719455415,\n",
       " 0.7374707559456254,\n",
       " 0.74297372748502,\n",
       " 0.7520603788764451,\n",
       " 0.7611352187669402,\n",
       " 0.7504983766436694,\n",
       " 0.7726232916179511,\n",
       " 0.7518791620053311,\n",
       " 0.7462100046542061,\n",
       " 0.749525969886367,\n",
       " 0.7087113682327632,\n",
       " 0.7419425083500687,\n",
       " 0.7531597109093799,\n",
       " 0.7557564078568942,\n",
       " 0.7760121900687671,\n",
       " 0.7322712953928178,\n",
       " 0.7531500965332527,\n",
       " 0.7586384996852904,\n",
       " 0.7554800128692996,\n",
       " 0.7338049792150618,\n",
       " 0.7449680523030615,\n",
       " 0.7338024213563189,\n",
       " 0.7538921474722581,\n",
       " 0.7711438208672928,\n",
       " 0.7363349008120152,\n",
       " 0.742233381079622,\n",
       " 0.7388706973418919,\n",
       " 0.7482345990196977,\n",
       " 0.7588163708242923,\n",
       " 0.7197892106812946,\n",
       " 0.7839738225180246,\n",
       " 0.750284851031134,\n",
       " 0.7715871735394473,\n",
       " 0.767593824932078,\n",
       " 0.7484465955028234,\n",
       " 0.7454237539279831,\n",
       " 0.7401352128671591,\n",
       " 0.7403300244689882,\n",
       " 0.7581750000969288,\n",
       " 0.7414184779283293,\n",
       " 0.7529169985229279,\n",
       " 0.7600150547780687,\n",
       " 0.7718464426183197,\n",
       " 0.7415572324971091,\n",
       " 0.767271428150684,\n",
       " 0.7545770499216182,\n",
       " 0.7433491492308597,\n",
       " 0.7581892917607186,\n",
       " 0.738309937608962,\n",
       " 0.7334050546606183,\n",
       " 0.7611036757625201,\n",
       " 0.76633997680324,\n",
       " 0.7418705258234847,\n",
       " 0.7459196786107808,\n",
       " 0.7672882332347625,\n",
       " 0.7871956688982388,\n",
       " 0.7501463156297592,\n",
       " 0.759730034700288,\n",
       " 0.7314657437911627,\n",
       " 0.7488309767602617,\n",
       " 0.7615245877058547,\n",
       " 0.7347211483613013,\n",
       " 0.749133267519917,\n",
       " 0.744351930513386,\n",
       " 0.7553758625406144,\n",
       " 0.7421685252333412,\n",
       " 0.7503962554394862,\n",
       " 0.7733300361659203,\n",
       " 0.7554494897185994,\n",
       " 0.7735841614528892,\n",
       " 0.7649617620834698,\n",
       " 0.7562574483150057,\n",
       " 0.7792591734105654,\n",
       " 0.7650080937510028,\n",
       " 0.7539906083711655,\n",
       " 0.7536613081536504,\n",
       " 0.7502420204808098,\n",
       " 0.7533852169956421,\n",
       " 0.7507640922062133,\n",
       " 0.7667759631827074,\n",
       " 0.7616280158144383,\n",
       " 0.7478136247159572,\n",
       " 0.7424503345410662,\n",
       " 0.7445390838317597,\n",
       " 0.7512261603084447,\n",
       " 0.7479939798703376,\n",
       " 0.7810516455346833,\n",
       " 0.748211201384992,\n",
       " 0.7451820971179127,\n",
       " 0.7762111564801206,\n",
       " 0.7599762151308322,\n",
       " 0.7499574468268997,\n",
       " 0.7497631792235079,\n",
       " 0.7574202548566424,\n",
       " 0.7503821268419096,\n",
       " 0.7592910051164671,\n",
       " 0.7485557073291005,\n",
       " 0.7628645187347539,\n",
       " 0.759074789722665,\n",
       " 0.7571343917632891,\n",
       " 0.7436921069686542,\n",
       " 0.7451952110022672,\n",
       " 0.7434842035337733,\n",
       " 0.7253943258119475,\n",
       " 0.7608605832957751,\n",
       " 0.7481995659261444,\n",
       " 0.7318937097431673,\n",
       " 0.7530914500442135,\n",
       " 0.7591829597308345,\n",
       " 0.7276229205032283,\n",
       " 0.7504752416340967,\n",
       " 0.7359108728567704,\n",
       " 0.7520386439542625,\n",
       " 0.7579087869694262,\n",
       " 0.7526784608869928,\n",
       " 0.7408766538460115,\n",
       " 0.7536930078922816,\n",
       " 0.7556763820078592,\n",
       " 0.7412695779320108,\n",
       " 0.7564013245330632,\n",
       " 0.7609002952462848,\n",
       " 0.7669968730448999,\n",
       " 0.7497243132279926,\n",
       " 0.7497913655955973,\n",
       " 0.7672655693184602,\n",
       " 0.777662448994863,\n",
       " 0.7602559315385243,\n",
       " 0.7457043321696946,\n",
       " 0.7588499640980609,\n",
       " 0.7641637918831847,\n",
       " 0.7537247083961152,\n",
       " 0.7180284774033888,\n",
       " 0.7605264791073931,\n",
       " 0.743258494276228,\n",
       " 0.7462280219631767,\n",
       " 0.7502437814299795,\n",
       " 0.7574310566815357,\n",
       " 0.7662502904877335,\n",
       " 0.7480450337709965,\n",
       " 0.7499559498048736,\n",
       " 0.7219489607807419,\n",
       " 0.7584532068644851,\n",
       " 0.7695458222676397,\n",
       " 0.7479721221061534,\n",
       " 0.7656883632432872,\n",
       " 0.7628912180806433,\n",
       " 0.741987788217352,\n",
       " 0.7625837559014996,\n",
       " 0.7693115937212625,\n",
       " 0.7317786648471972,\n",
       " 0.7573286963254423,\n",
       " 0.7176394302493557,\n",
       " 0.7512349941155232,\n",
       " 0.7447840257356307,\n",
       " 0.7441987104249917,\n",
       " 0.728518866584748,\n",
       " 0.7443234697240981,\n",
       " 0.7663427812413582,\n",
       " 0.7552584413132679,\n",
       " 0.7709610217380991,\n",
       " 0.7584295169317774,\n",
       " 0.7339922380036826,\n",
       " 0.7735665901363469,\n",
       " 0.7565507561331123,\n",
       " 0.7380414506198741,\n",
       " 0.7450187837870577,\n",
       " 0.7645054600648576,\n",
       " 0.7364180208727554,\n",
       " 0.7636641976900277,\n",
       " 0.7530295294368966,\n",
       " 0.7486553219039973,\n",
       " 0.7635013734336122,\n",
       " 0.7649620997851894,\n",
       " 0.7458112552366407,\n",
       " 0.7637580563288573,\n",
       " 0.7643599876010062,\n",
       " 0.7645824773337804,\n",
       " 0.7500680789783493,\n",
       " 0.7408798057287854,\n",
       " 0.7455836122014742,\n",
       " 0.7200556225066068,\n",
       " 0.7581012773387,\n",
       " 0.7583841211047943,\n",
       " 0.7536940693197358,\n",
       " 0.7407050242740392,\n",
       " 0.7568303253884588,\n",
       " 0.7500900135350006,\n",
       " 0.7561306783331794,\n",
       " 0.7576242770042355,\n",
       " 0.7404475367958956,\n",
       " 0.7507634942809247,\n",
       " 0.7581982529552307,\n",
       " 0.7436336343182789,\n",
       " 0.758097668951206,\n",
       " 0.756316473552346,\n",
       " 0.7643188346670979,\n",
       " 0.7696876798626696,\n",
       " 0.7584655712701854,\n",
       " 0.7662116274333163,\n",
       " 0.7518402516895155,\n",
       " 0.7599596150818103,\n",
       " 0.760163143981804,\n",
       " 0.7747336382426583,\n",
       " 0.7305309100451308,\n",
       " 0.7454247034617214,\n",
       " 0.7550746495629743,\n",
       " 0.7774060324670041,\n",
       " 0.7715192339444183,\n",
       " 0.7417727364644405,\n",
       " 0.7412076764189481,\n",
       " 0.7617546964947164,\n",
       " 0.7532887088185464,\n",
       " 0.7474156544476239,\n",
       " 0.7330827905423267,\n",
       " 0.7771772369029729,\n",
       " 0.7498489126677828,\n",
       " 0.7604455555216025,\n",
       " 0.7319733925051287,\n",
       " 0.7421214097875884,\n",
       " 0.7631460739162564,\n",
       " 0.7685555049721319,\n",
       " 0.7729514764383058,\n",
       " 0.770374415297155,\n",
       " 0.7584735192039843,\n",
       " 0.743966199225604,\n",
       " 0.736565730757986,\n",
       " 0.7528220083035639,\n",
       " 0.7791533506150474,\n",
       " 0.7646584009128825,\n",
       " 0.7509367840136952,\n",
       " 0.7493426302991377,\n",
       " 0.7615932721634681,\n",
       " 0.7386375337754583,\n",
       " 0.7783400476041786,\n",
       " 0.774323357227266,\n",
       " 0.7647347904929936,\n",
       " 0.7511084867401915,\n",
       " 0.7629173209619865,\n",
       " 0.7381967318027058,\n",
       " 0.7551809642012969,\n",
       " 0.7567197391294275,\n",
       " 0.7257877101701345,\n",
       " 0.7539933038258286,\n",
       " 0.7462714274343731,\n",
       " 0.7633097624966064,\n",
       " 0.7841895319703742,\n",
       " 0.7328339402474451,\n",
       " 0.743431912069691,\n",
       " 0.7334185366307633,\n",
       " 0.7282062638656251,\n",
       " 0.7640441441247832,\n",
       " 0.7447307335797511,\n",
       " 0.7803377428823614,\n",
       " 0.7723826853885706,\n",
       " 0.7325353685746084,\n",
       " 0.7608306963892576,\n",
       " 0.7283101464709104,\n",
       " 0.7650257661188159,\n",
       " 0.7734513220646485,\n",
       " 0.7699484335875135,\n",
       " 0.7752070208904344,\n",
       " 0.7466765892349124,\n",
       " 0.7457235732500703,\n",
       " 0.7480546969058565,\n",
       " 0.7504168640494169,\n",
       " 0.7404803473233412,\n",
       " 0.7595261809302851,\n",
       " 0.7250254504857115,\n",
       " 0.7451418657740739,\n",
       " 0.7700261524640906,\n",
       " 0.7649269461103907,\n",
       " 0.7591170817453765,\n",
       " 0.7614352260770539,\n",
       " 0.7438493935435684,\n",
       " 0.7588683338376373,\n",
       " 0.7627756632247951,\n",
       " 0.7606063387547102,\n",
       " 0.7549196453331971,\n",
       " 0.735885075319678,\n",
       " 0.7396019514718579,\n",
       " 0.7566313972876124,\n",
       " 0.7702811013630503,\n",
       " 0.758316226834489,\n",
       " 0.7794055213489643,\n",
       " 0.7645514427829363,\n",
       " 0.7464694953898814,\n",
       " 0.7242107920106616,\n",
       " 0.7351781999382377,\n",
       " 0.7567203443732965,\n",
       " 0.7472682697352814,\n",
       " 0.7612009905900308,\n",
       " 0.7569138522227449,\n",
       " 0.7306693234574129,\n",
       " 0.7627837650981906,\n",
       " 0.7638129672699291,\n",
       " 0.7415263631844404,\n",
       " 0.7393434533677866,\n",
       " 0.7522390182121305,\n",
       " 0.7779058771020017,\n",
       " 0.7463626479477945,\n",
       " 0.7666055596976253,\n",
       " 0.7344562229706795,\n",
       " 0.754237828772593,\n",
       " 0.7628549388601823,\n",
       " 0.7554993319450105,\n",
       " 0.7657856786768135,\n",
       " 0.747312560013611,\n",
       " 0.7245617076039412,\n",
       " 0.7320434441468113,\n",
       " 0.7558460880961021,\n",
       " 0.7334717824327919,\n",
       " 0.7568177709456241,\n",
       " 0.7397453621438295,\n",
       " 0.7726055137662855,\n",
       " 0.7303087942739346,\n",
       " 0.7498026007792142,\n",
       " 0.7634516488173871,\n",
       " 0.7708332337222177,\n",
       " 0.7528392495109123,\n",
       " 0.7560510176407802,\n",
       " 0.7546000334643664,\n",
       " 0.7384510175074713,\n",
       " 0.7476439796335043,\n",
       " 0.7351380885238059,\n",
       " 0.7465567315697608,\n",
       " 0.731459011807351,\n",
       " 0.7541318793565183,\n",
       " 0.7474644423643497,\n",
       " 0.7523904299510551,\n",
       " 0.7687218391768307,\n",
       " 0.7428780033673968,\n",
       " 0.7413985072018587,\n",
       " 0.7666236216915792,\n",
       " 0.7480671709212187,\n",
       " 0.762100704827842,\n",
       " 0.7306905279613892,\n",
       " 0.712802674089689,\n",
       " 0.7572235137187605,\n",
       " 0.7576813517556489,\n",
       " 0.7524991837145447,\n",
       " 0.7390948656626967,\n",
       " 0.7327790471091595,\n",
       " 0.7625406619602709,\n",
       " 0.7667312712903941,\n",
       " 0.7525682204004978,\n",
       " 0.7633354364642251,\n",
       " 0.7524012946052565,\n",
       " 0.7625796591150762,\n",
       " 0.7641361096873696,\n",
       " 0.7484426730629281,\n",
       " 0.7606669392421611,\n",
       " 0.7419886211423379,\n",
       " 0.7727982041643728,\n",
       " 0.7660528652257811,\n",
       " 0.7679736921203542,\n",
       " 0.7374056132898303,\n",
       " 0.7301633636037832,\n",
       " 0.7487974044165644,\n",
       " 0.761035930598499,\n",
       " 0.7591244159800303,\n",
       " 0.7431558302719689,\n",
       " 0.745652915837601,\n",
       " 0.7632806249508273,\n",
       " 0.7576614008269599,\n",
       " 0.7248858845418381,\n",
       " 0.7604927106847494,\n",
       " 0.7409858220078956,\n",
       " 0.7443598942806712,\n",
       " 0.7596785492612482,\n",
       " 0.7409734170934189,\n",
       " 0.7606513004670024,\n",
       " 0.7583325361463953,\n",
       " 0.7513681677433244,\n",
       " 0.7575616363593103,\n",
       " 0.7746687993466251,\n",
       " 0.7566487429766053,\n",
       " 0.7580521026019752,\n",
       " 0.7482373189565779,\n",
       " 0.7525032124918104,\n",
       " 0.7461575289907678,\n",
       " 0.7476715795672272,\n",
       " 0.7476841237827601,\n",
       " 0.7479178730699906,\n",
       " 0.7494341857677266,\n",
       " 0.7698420839759315,\n",
       " 0.7695113641166362,\n",
       " 0.7427649615333376,\n",
       " 0.7385742903679448,\n",
       " 0.7375629655412526,\n",
       " 0.760743716482644,\n",
       " 0.7553662316225874,\n",
       " 0.7458826566890878,\n",
       " 0.7690708216115965,\n",
       " 0.7532173373028322,\n",
       " 0.7759926536456151,\n",
       " 0.7631560008166187,\n",
       " 0.7563593474300566,\n",
       " 0.7486624639255469,\n",
       " 0.7298971942405479,\n",
       " 0.7647531656684948,\n",
       " 0.7324839149970691,\n",
       " 0.7640360248559743,\n",
       " 0.7479708542602725,\n",
       " 0.7619019028364133,\n",
       " 0.7384185986671119,\n",
       " 0.7515092263102802,\n",
       " 0.7476122160108113,\n",
       " 0.7508113154108872,\n",
       " 0.7439597159128747,\n",
       " 0.7430952856289033,\n",
       " 0.7434778529862025,\n",
       " 0.7393851852041797,\n",
       " 0.7534931029070471,\n",
       " 0.7427995977651426,\n",
       " 0.7756194687104944,\n",
       " 0.7292871248518599,\n",
       " 0.7538692246661304,\n",
       " 0.7344309958398395,\n",
       " 0.7538552478088694,\n",
       " 0.7632328822749043,\n",
       " 0.7401293952755038]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1s_bootstrap = []\n",
    "Test_X_array = np.array(Test_X)\n",
    "Test_Y_array = np.array(Test_Y_minist)\n",
    "\n",
    "for i in range(1000):\n",
    "    print(i)\n",
    "    bootstraped_test_x_index = np.random.choice(range(0,len(Test_X)), size=len(Test_X), replace=True)\n",
    "    bootstraped_test_x = np.array([Test_X_array[j] for j in bootstraped_test_x_index])\n",
    "    bootstraped_test_y = np.array([Test_Y_array[j] for j in bootstraped_test_x_index])\n",
    "    test_prediction = model.predict(bootstraped_test_x)\n",
    "    f1s_bootstrap.append(f1_score(y_true=np.array(bootstraped_test_y), y_pred=predict_classes(test_prediction), average='macro'))\n",
    "f1s_bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7248777801183908\n",
      "0.7771780962013661 \n",
      "\n",
      "0.7266743015756976\n",
      "0.7774996621403694\n"
     ]
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "print(np.percentile(f1s_bootstrap, 2.5))\n",
    "print(np.percentile(f1s_bootstrap, 97.5), \"\\n\")\n",
    "print(np.array(f1s_bootstrap).mean() - np.array(f1s_bootstrap).std(ddof=1)*1.96)\n",
    "print(np.array(f1s_bootstrap).mean() + np.array(f1s_bootstrap).std(ddof=1)*1.96)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7520869818580335 +- 0.025399970764020383\n"
     ]
    }
   ],
   "source": [
    "print(np.array(f1s_bootstrap).mean(), \"+-\", np.array(f1s_bootstrap).std()*1.96)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of HAN attetion weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "from keras.models import Sequential\n",
    "\n",
    "# Creating a reverse dictionary\n",
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "# Replicates the Attention mechanism and returns the weight coefficient for a given word or sentence.\n",
    "def wordAttentionWeights(sequenceSentence,weights):\n",
    "    \"\"\"\n",
    "    The same function as the AttentionLayer class.\n",
    "    \"\"\"\n",
    "    uit = np.dot(sequenceSentence, weights[0]) + weights[1]\n",
    "    uit = np.tanh(uit)\n",
    "\n",
    "    ait = np.dot(uit, weights[2])\n",
    "    ait = np.squeeze(ait)\n",
    "    ait = np.exp(ait)\n",
    "    ait /= np.sum(ait)\n",
    "    \n",
    "    return ait\n",
    "\n",
    "def from_y_to_class_names(indices, classes_names):\n",
    "    names = \"\"\n",
    "    for i, j in enumerate(indices):\n",
    "        if(j == 1):\n",
    "            names+=classes_names[i]+\" | \"\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decreto de índice 1261 do dataset de testes:\n",
      "\n",
      "Expected Classification: Saúde | \n",
      "Predicted Classification: Saúde |  \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       " dá <span style='background: rgba(100,200,100,0.82)'>nova</span> redação <span style='background: rgba(100,200,100,0.3999999999999999)'>a</span> dispositivos <span style='background: rgba(100,200,100,0.74)'>do</span> anexo ao decreto 3675 de 28 11 2000 que dispõe sobre medidas especiais relacionadas com o registro de medicamentos genéricos de que trata o art 4 <span style='background: rgba(100,200,100,0.47999999999999987)'>da</span> lei 9787 <span style='background: rgba(100,200,100,0.78)'>de</span> <span style='background: rgba(100,200,100,0.7999999999999999)'>10</span> 02 <span style='background: rgba(100,200,100,0.5599999999999999)'>1999</span>. o presidente da república no uso da atribuição que lhe confere o art 84 inciso iv da constituição e tendo em vista o disposto no art 4 da lei n 9787 de 10 de fevereiro de 1999 decreta. <span style='background: rgba(100,200,100,0.7)'>art</span> <span style='background: rgba(100,200,100,0.5999999999999999)'>1</span> <span style='background: rgba(100,200,100,0.6599999999999999)'>os</span> <span style='background: rgba(100,200,100,0.6799999999999999)'>dispositivos</span> <span style='background: rgba(100,200,100,0.5399999999999999)'>indicados</span> <span style='background: rgba(100,200,100,0.6199999999999999)'>do</span> <span style='background: rgba(100,200,100,0.33999999999999986)'>anexo</span> ao decreto n 3675 de 28 <span style='background: rgba(100,200,100,0.84)'>de</span> <span style='background: rgba(100,200,100,0.9)'>novembro</span> <span style='background: rgba(100,200,100,0.72)'>de</span> 2000 passam a vigorar com a seguinte redação. <span style='background: rgba(100,200,100,0.3599999999999999)'>anexo</span>. <span style='background: rgba(100,200,100,0.9199999999999999)'>iii</span> relatório <span style='background: rgba(100,200,100,0.76)'>técnico</span>.<span style='background: rgba(255,255,0,0.5)'> <span style='background: rgba(100,200,100,0.94)'>b</span> <span style='background: rgba(100,200,100,0.98)'>aspectos</span> <span style='background: rgba(100,200,100,1.0)'>do</span> <span style='background: rgba(100,200,100,0.96)'>controle</span> <span style='background: rgba(100,200,100,0.86)'>de</span> <span style='background: rgba(100,200,100,0.88)'>qualidade</span>.</span> 4. caso o medicamento <span style='background: rgba(100,200,100,0.3799999999999999)'>de</span> referência utilizado nos ensaios não seja da <span style='background: rgba(100,200,100,0.41999999999999993)'>mesma</span> empresa do medicamento <span style='background: rgba(100,200,100,0.5199999999999999)'>de</span> referência nacional ou de empresa licenciada desta <span style='background: rgba(100,200,100,0.2799999999999998)'>a</span> empresa interessada <span style='background: rgba(100,200,100,0.6399999999999999)'>no</span> <span style='background: rgba(100,200,100,0.4999999999999999)'>registro</span> deverá apresentar além <span style='background: rgba(100,200,100,0.31999999999999984)'>do</span> <span style='background: rgba(100,200,100,0.21999999999999986)'>certificado</span> <span style='background: rgba(100,200,100,0.2599999999999999)'>de</span> equivalência farmacêutica o estudo comparativo dos. nr art 2 <span style='background: rgba(100,200,100,0.45999999999999985)'>este</span> decreto entra <span style='background: rgba(100,200,100,0.23999999999999988)'>em</span> vigor na data de sua publicação. brasília 3 de janeiro de 2001. 180 <span style='background: rgba(100,200,100,0.2999999999999998)'>da</span> independência e 113 <span style='background: rgba(100,200,100,0.5799999999999998)'>da</span> <span style='background: rgba(100,200,100,0.43999999999999995)'>república</span>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_n = 1261 # Selects the number of the document in the test set\n",
    "\n",
    "document_test = np.array(list(map(sequence_to_text, Test_X[test_n])))\n",
    "\n",
    "# Word submodel\n",
    "hidden_word_encoding_out = Model(inputs=model.get_layer(\"sent_linking\").layer.input, outputs=model.get_layer(\"sent_linking\").layer.get_layer(\"word_gru\").output)\n",
    "word_context = model.get_layer(\"sent_linking\").layer.get_layer(\"attention_with_context\").get_weights()  #weight, biass, u\n",
    "\n",
    "# Sentence submodel\n",
    "hidden_sentence_encoding_out = Sequential()\n",
    "for layer in model.layers[:-2]:\n",
    "    hidden_sentence_encoding_out.add(layer)\n",
    "# print(hidden_sentence_encoding_out.summary())\n",
    "sentence_context = model.get_layer(\"attention_with_context_1\").get_weights()\n",
    "\n",
    "hidden_sentence_encodings = hidden_sentence_encoding_out.predict(np.array([Test_X[test_n]]))\n",
    "ait_sentence = wordAttentionWeights(hidden_sentence_encodings,sentence_context)\n",
    "\n",
    "# Visualization\n",
    "not_none_index = []\n",
    "not_none_sentences_index = []\n",
    "full_phrases = []\n",
    "full_ait_words = []\n",
    "for j in range(len(ait_sentence)):\n",
    "    hidden_word_encodings = hidden_word_encoding_out.predict(np.array([Test_X[test_n][j]]))\n",
    "    ait_word = wordAttentionWeights(hidden_word_encodings,word_context)\n",
    "    if(np.sum(document_test[j] == None) != len(document_test[j])):  # checks if it's not a full none sentence.\n",
    "        not_none_index = [i for i in range(len(document_test[j])) if document_test[j][i] != None]\n",
    "        ait_word*=np.sqrt(ait_sentence[j])\n",
    "        ait_word = ait_word[not_none_index]  \n",
    "        phrase = document_test[j][not_none_index]\n",
    "\n",
    "        full_phrases.append(phrase)\n",
    "        full_ait_words.append(ait_word)\n",
    "\n",
    "        not_none_sentences_index.append(j)\n",
    "\n",
    "full_ait_words_flatten = []\n",
    "for i in full_ait_words:\n",
    "    for j in i:\n",
    "        full_ait_words_flatten.append(j)\n",
    "full_ait_words_flatten = np.array(full_ait_words_flatten)\n",
    "important_indexes = full_ait_words_flatten.argsort()[::-1]\n",
    "\n",
    "most_important_sentence = ait_sentence[not_none_sentences_index].argsort()[::-1][0]\n",
    "\n",
    "show_important_words = 40   # first n important words to show\n",
    "\n",
    "full_document = \"\"\n",
    "counter = 0\n",
    "counter_phrase = 0\n",
    "for i in full_phrases:\n",
    "    if(counter_phrase == most_important_sentence):\n",
    "        full_document+=(\"<span style='background: rgba(255,255,0,0.5)'>\")\n",
    "\n",
    "    for j in i:\n",
    "        if(counter not in important_indexes[:show_important_words]):\n",
    "            full_document+=(\" \"+j)\n",
    "        else:\n",
    "            transparency = 1-((1/show_important_words)*0.8)*np.where(important_indexes==counter)[0][0]\n",
    "            full_document+=(\" <span style='background: rgba(100,200,100,\"+str(transparency)+\")'>\"+j+\"</span>\")\n",
    "        counter+=1\n",
    "    full_document+=\".\"\n",
    "\n",
    "    if(counter_phrase == most_important_sentence):\n",
    "        full_document+=(\"</span>\")\n",
    "    counter_phrase+=1\n",
    "\n",
    "print(\"Decreto de índice \"+str(test_n)+\" do dataset de testes:\\n\")\n",
    "print(\"Expected Classification:\",from_y_to_class_names(Test_Y_sen[test_n], sen_classes_names))\n",
    "print(\"Predicted Classification:\",from_y_to_class_names(predict_classes(test_prediction)[test_n], sen_classes_names),\"\\n\")\n",
    "\n",
    "display(HTML(full_document))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
